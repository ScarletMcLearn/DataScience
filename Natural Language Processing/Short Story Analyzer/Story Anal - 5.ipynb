{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# docs = pickle.load( open( \"Resources/story_list.pkl\", \"rb\" ) )\n",
    "\n",
    "pos = pickle.load(open('Resources/pos_word_list.pkl', 'rb'))\n",
    "\n",
    "neg = pickle.load(open('Resources/neg_word_list.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from StoryAnalHelper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "mypath = '/home/scarket/Project/Data Science/NLP/Short Story Analyzer/Resources'\n",
    "\n",
    "onlyfiles = list_files(mypath)\n",
    "\n",
    "\n",
    "\n",
    "i = 0\n",
    "\n",
    "while (i<len(onlyfiles)):\n",
    "    if not onlyfiles[i].endswith('.txt'):\n",
    "#         print(onlyfiles[i])\n",
    "        onlyfiles.remove(onlyfiles[i])\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "# mypath = '/home/scarket/Project/Data Science/NLP/Short Story Analyzer/Resources'\n",
    "\n",
    "def list_files(mypath):\n",
    "    return [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "\n",
    "\n",
    "def remove_punc(tmp_fle):\n",
    "    punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
    "\n",
    "    tmp_fle = tmp_fle.lower().strip().replace('\\n', ' ').replace('\\r', ' ')\n",
    "\n",
    "    tmp_file_no_punc = \"\"\n",
    "\n",
    "    for char in tmp_fle:\n",
    "       if char not in punctuations:\n",
    "           tmp_file_no_punc = tmp_file_no_punc + char\n",
    "            \n",
    "    return tmp_file_no_punc\n",
    "\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "def lemmatizer(text):        \n",
    "    sent = []\n",
    "    doc = nlp(text)\n",
    "    for word in doc:\n",
    "        sent.append(word.lemma_)\n",
    "    return \" \".join(sent).replace('-PRON-', '')\n",
    "\n",
    "\n",
    "def pos_neg_stat(doc):\n",
    "    '''doc == str '''\n",
    "    neg_occ = 0\n",
    "    pos_occ = 0\n",
    "    \n",
    "    pos_ = []\n",
    "    neg_ = []\n",
    "\n",
    "    for _ in (doc.split()):\n",
    "        if _ in pos:\n",
    "            pos_occ = pos_occ + 1\n",
    "            pos_.append(_)\n",
    "\n",
    "        elif _ in neg:\n",
    "            neg_occ = neg_occ + 1\n",
    "            neg_.append(_)\n",
    "\n",
    "    print(\"Positives: \" + str(pos_occ) + '/' + str(len(doc.split())))\n",
    "    print(\"Negatives: \" + str(neg_occ) + '/' + str(len(doc.split())))\n",
    "    \n",
    "    doc_l = len(doc.split())\n",
    "    pos_scr = len(pos_)/doc_l * 100\n",
    "    neg_scr = len(neg_)/doc_l * 100\n",
    "    \n",
    "    print('Positive Percentage: ' + str(pos_scr))\n",
    "    print('Negative Percentage: ' + str(neg_scr))\n",
    "    \n",
    "    return({'pos':pos_, 'neg':neg_, 'pos_scr':pos_scr, 'neg_scr':neg_scr, 'doc_len':doc_l, 'pos_frac':(\"Positives: \" + str(pos_occ) + '/' + str(len(doc.split()))), 'neg_frac':(\"Negatives: \" + str(neg_occ) + '/' + str(len(doc.split())))})\n",
    "\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "def save_senty_count(file_name, sl):\n",
    "    with open(str(file_name)+'.json', 'w') as outfile:  \n",
    "        json.dump(sl, outfile)\n",
    "        print('Wrote Json!')\n",
    "        \n",
    "\n",
    "def read_senty_count(file_name):\n",
    "    with open(str(file_name)+'.json') as json_file:  \n",
    "        return(json.load(json_file))\n",
    "    \n",
    "    \n",
    "    \n",
    "def m_emo_to_senti_percentage(posi, negi):\n",
    "    if posi > negi:\n",
    "        p = posi*100/(posi+negi)\n",
    "        print('Positive: ' + str(p))\n",
    "        return p\n",
    "    n = negi*100/(posi+negi)\n",
    "    print('Negative: ' + str(n))\n",
    "    return n\n",
    "\n",
    "\n",
    "\n",
    "def get_cmn_in_range(lower, upper, sorted_count):\n",
    "    tmp = []\n",
    "    \n",
    "    for itm in sorted_count.most_common(upper):\n",
    "        if itm not in sorted_count.most_common(lower):\n",
    "            tmp.append(itm)\n",
    "#             print(itm)\n",
    "    return tmp\n",
    "\n",
    "\n",
    "\n",
    "def pos_tag(word):\n",
    "    return nlp(word)[0].pos_\n",
    "\n",
    "\n",
    "\n",
    "def res_lst(tmp):\n",
    "    \"\"\"tmp == list\"\"\"\n",
    "    c_res = []\n",
    "\n",
    "    for i in range(len(tmp)):\n",
    "        wrd = str(tmp[i])\n",
    "        p_tag = str(pos_tag(tmp[i]))\n",
    "        s_count = str(sorted_count[tmp[i]])\n",
    "\n",
    "        c_res.append([wrd, p_tag, s_count])\n",
    "\n",
    "        print(wrd + \" : \" + (p_tag) + \" : \" + s_count)\n",
    "        \n",
    "    return c_res\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def pos_count(c_rest):\n",
    "    pos_cnt = {}\n",
    "\n",
    "    for i in c_res:\n",
    "        if i[1] not in pos_cnt.keys():\n",
    "            pos_cnt[i[1]] = 1\n",
    "        else:\n",
    "            pos_cnt[i[1]] = 1 + pos_cnt[i[1]]\n",
    "            \n",
    "    return pos_cnt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def w_2_fle(the_filename, my_list):\n",
    "    with open(the_filename, 'w') as f:\n",
    "        for s in my_list:\n",
    "            f.write(str(s) + '\\n')\n",
    "    print('Done!')\n",
    "\n",
    "def r_f_fle(the_filename):\n",
    "    with open(the_filename, 'r') as f:\n",
    "        return [line.rstrip('\\n') for line in f]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "# mypath = '/home/scarket/Project/Data Science/NLP/Short Story Analyzer/Resources'\n",
    "\n",
    "def list_files(mypath):\n",
    "    return [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "\n",
    "\n",
    "def remove_punc(tmp_fle):\n",
    "    punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
    "\n",
    "    tmp_fle = tmp_fle.lower().strip().replace('\\n', ' ').replace('\\r', ' ')\n",
    "\n",
    "    tmp_file_no_punc = \"\"\n",
    "\n",
    "    for char in tmp_fle:\n",
    "       if char not in punctuations:\n",
    "           tmp_file_no_punc = tmp_file_no_punc + char\n",
    "            \n",
    "    return tmp_file_no_punc\n",
    "\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "def lemmatizer(text):        \n",
    "    sent = []\n",
    "    doc = nlp(text)\n",
    "    for word in doc:\n",
    "        sent.append(word.lemma_)\n",
    "    return \" \".join(sent).replace('-PRON-', '')\n",
    "\n",
    "\n",
    "def pos_neg_stat(doc):\n",
    "    '''doc == str '''\n",
    "    neg_occ = 0\n",
    "    pos_occ = 0\n",
    "    \n",
    "    pos_ = []\n",
    "    neg_ = []\n",
    "\n",
    "    for _ in (doc.split()):\n",
    "        if _ in pos:\n",
    "            pos_occ = pos_occ + 1\n",
    "            pos_.append(_)\n",
    "\n",
    "        elif _ in neg:\n",
    "            neg_occ = neg_occ + 1\n",
    "            neg_.append(_)\n",
    "\n",
    "#     print(\"Positives: \" + str(pos_occ) + '/' + str(len(doc.split())))\n",
    "#     print(\"Negatives: \" + str(neg_occ) + '/' + str(len(doc.split())))\n",
    "    \n",
    "    doc_l = len(doc.split())\n",
    "    pos_scr = len(pos_)/doc_l * 100\n",
    "    neg_scr = len(neg_)/doc_l * 100\n",
    "    \n",
    "#     print('Positive Percentage: ' + str(pos_scr))\n",
    "#     print('Negative Percentage: ' + str(neg_scr))\n",
    "    \n",
    "    return({'pos':pos_, 'neg':neg_, 'pos_scr':pos_scr, 'neg_scr':neg_scr, 'doc_len':doc_l, 'pos_frac':(\"Positives: \" + str(pos_occ) + '/' + str(len(doc.split()))), 'neg_frac':(\"Negatives: \" + str(neg_occ) + '/' + str(len(doc.split())))})\n",
    "\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "def save_senty_count(file_name, sl):\n",
    "    with open(str(file_name)+'.json', 'w') as outfile:  \n",
    "        json.dump(sl, outfile)\n",
    "#         print('Wrote Json!')\n",
    "        \n",
    "\n",
    "def read_senty_count(file_name):\n",
    "    with open(str(file_name)+'.json') as json_file:  \n",
    "        return(json.load(json_file))\n",
    "    \n",
    "    \n",
    "    \n",
    "def m_emo_to_senti_percentage(posi, negi):\n",
    "    if posi > negi:\n",
    "        p = posi*100/(posi+negi)\n",
    "#         print('Positive: ' + str(p))\n",
    "        return p\n",
    "    n = negi*100/(posi+negi)\n",
    "#     print('Negative: ' + str(n))\n",
    "    return n\n",
    "\n",
    "\n",
    "\n",
    "def get_cmn_in_range(lower, upper, sorted_count):\n",
    "    tmp = []\n",
    "    \n",
    "    for itm in sorted_count.most_common(upper):\n",
    "        if itm not in sorted_count.most_common(lower):\n",
    "            tmp.append(itm)\n",
    "#             print(itm)\n",
    "    return tmp\n",
    "\n",
    "\n",
    "\n",
    "def pos_tag(word):\n",
    "    return nlp(word)[0].pos_\n",
    "\n",
    "\n",
    "\n",
    "def res_lst(tmp):\n",
    "    \"\"\"tmp == list\"\"\"\n",
    "    c_res = []\n",
    "\n",
    "    for i in range(len(tmp)):\n",
    "        wrd = str(tmp[i])\n",
    "        p_tag = str(pos_tag(tmp[i]))\n",
    "        s_count = str(sorted_count[tmp[i]])\n",
    "\n",
    "        c_res.append([wrd, p_tag, s_count])\n",
    "\n",
    "#         print(wrd + \" : \" + (p_tag) + \" : \" + s_count)\n",
    "        \n",
    "    return c_res\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def pos_count(c_rest):\n",
    "    pos_cnt = {}\n",
    "\n",
    "    for i in c_res:\n",
    "        if i[1] not in pos_cnt.keys():\n",
    "            pos_cnt[i[1]] = 1\n",
    "        else:\n",
    "            pos_cnt[i[1]] = 1 + pos_cnt[i[1]]\n",
    "            \n",
    "    return pos_cnt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def w_2_fle(the_filename, my_list):\n",
    "    with open(the_filename, 'w') as f:\n",
    "        for s in my_list:\n",
    "            f.write(str(s) + '\\n')\n",
    "#     print('Done!')\n",
    "\n",
    "def r_f_fle(the_filename):\n",
    "    with open(the_filename, 'r') as f:\n",
    "        return [line.rstrip('\\n') for line in f]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3+3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with file : A Holiday Task.txt\n",
      "Working with file : The Last Leaf.txt\n",
      "Working with file : The Selfish Giant.txt\n",
      "Working with file : A Lickpenny Lover.txt\n",
      "Working with file : The Open Window.txt\n",
      "Working with file : Upturned Face.txt\n",
      "Working with file : The Necklace.txt\n",
      "Working with file : What Christmas Is As We Grow Older.txt\n",
      "Working with file : The Bet.txt\n",
      "Working with file : The Cop and the Anthem.txt\n",
      "Working with file : The Cask of Amontillado - Edgar Allan Poe.txt\n",
      "Working with file : The Terrible Old Man.txt\n",
      "Working with file : The Sphinx Without a Secret.txt\n",
      "Working with file : A Respectable Woman.txt\n",
      "Working with file : A Duel.txt\n",
      "Working with file : The Man In The Moon.txt\n",
      "Working with file : The Nice People.txt\n",
      "Working with file : A Defensive Diamond.txt\n",
      "Working with file : The Tell-Tale Heart.txt\n",
      "Working with file : Sredni Vashtar.txt\n",
      "Working with file : The Veteran.txt\n",
      "Working with file : The Disappearance of Crispina Umberleigh.txt\n",
      "Working with file : A Cosmopolite in a Cafe.txt\n",
      "Working with file : The Merino Sheep.txt\n",
      "Working with file : The Vendetta.txt\n",
      "Working with file : The Little Match Girl.txt\n",
      "Working with file : A Defenseless Creature.txt\n",
      "Working with file : The Log.txt\n",
      "Working with file : About Love.txt\n",
      "Working with file : The Yarkand Manner.txt\n",
      "Working with file : A Journey.txt\n",
      "Working with file : The Dreamer.txt\n",
      "Working with file : Witches' Loaves.txt\n",
      "Working with file : Ex Oblivione.txt\n",
      "Working with file : The Colonel's Ideas.txt\n",
      "Working with file : After the Race.txt\n",
      "Working with file : The Veldt - Ray Bradbury.txt\n",
      "Working with file : Boule de Suif.txt\n",
      "Working with file : The Diary of a Madman.txt\n",
      "Working with file : The Pit and the Pendulum.txt\n",
      "Working with file : Hyacinth.txt\n",
      "Working with file : The Death Of A Government Clerk.txt\n",
      "Working with file : Bertie's Christmas Eve.txt\n",
      "Working with file : The Shoemaker And The Devil.txt\n",
      "Working with file : The Unkindest Blow.txt\n",
      "Working with file : Esme.txt\n",
      "Working with file : The Luck of Roaring Camp.txt\n",
      "Working with file : What You Want.txt\n",
      "Working with file : A True Story, Repeated Word for Word As I Heard It.txt\n",
      "Working with file : The Cripple.txt\n",
      "Working with file : The Nightingale and the Rose.txt\n",
      "Working with file : A Dead Woman's Secret.txt\n",
      "Working with file : The Game.txt\n",
      "Working with file : Gabriel-Ernest.txt\n",
      "Working with file : The Model Millionaire.txt\n",
      "Working with file : The Man in the Brown Coat.txt\n",
      "Working with file : Springtime a la Carte.txt\n",
      "Working with file : The Furnished Room.txt\n",
      "Working with file : Rikki-Tikki-Tavi.txt\n",
      "Working with file : The Unrest-Cure.txt\n",
      "Working with file : My Kinsman, Major Molineux.txt\n",
      "Working with file : Hermann The Irascible.txt\n",
      "Working with file : Gentle Hand.txt\n",
      "Working with file : The Student.txt\n",
      "Working with file : The Lumber Room.txt\n",
      "Working with file : Hearts And Hands.txt\n",
      "Working with file : The Boarded Window.txt\n",
      "Working with file : According to Their Lights.txt\n",
      "Working with file : A Jury of Her Peers.txt\n",
      "Working with file : How the Leopard Got His Spots.txt\n",
      "Working with file : The Celebrated Jumping Frog of Calaveras County - Mark Twain.txt\n",
      "Working with file : The Story of Keesh.txt\n",
      "Working with file : A Vine on a House.txt\n",
      "Working with file : The Wolves of Cernogatz.txt\n",
      "Working with file : The Cask of Amontillado.txt\n",
      "Working with file : A New England Nun.txt\n",
      "Working with file : Transients in Arcadia.txt\n",
      "Working with file : The Romance of a Busy Broker.txt\n",
      "Working with file : A Father's Confession.txt\n",
      "Working with file : The Princess And The Puma.txt\n",
      "Working with file : Alexandre.txt\n",
      "Working with file : One Summer Night.txt\n",
      "Working with file : Turkeys Turning The Tables.txt\n",
      "Working with file : A Retrieved Reformation.txt\n",
      "Working with file : The Schartz-Metterklume Method.txt\n",
      "Working with file : The Child's Story.txt\n",
      "Working with file : On The Day of the Crucifixion.txt\n",
      "Working with file : Hands.txt\n",
      "Working with file : Aloha Oe.txt\n",
      "Working with file : A Telephone Call.txt\n",
      "Working with file : To Build a Fire.txt\n",
      "Working with file : The Masque of the Red Death.txt\n",
      "Working with file : The Last Fight In The Coliseum.txt\n",
      "Working with file : An Alpine Divorce.txt\n",
      "Working with file : The Affair at Coulter's Notch.txt\n",
      "Working with file : Christmas Every Day.txt\n",
      "Working with file : A Blunder.txt\n",
      "Working with file : Kew Gardens.txt\n",
      "Working with file : Two Friends.txt\n",
      "Working with file : Amy's Question.txt\n",
      "Working with file : Chickamauga.txt\n",
      "Working with file : The Hand.txt\n",
      "Working with file : The Call of Cthulhu.txt\n",
      "Working with file : A Baby Tramp.txt\n",
      "Working with file : The Cats of Ulthar.txt\n",
      "Working with file : The Huntsman.txt\n",
      "Working with file : Eveline.txt\n",
      "Working with file : Babes in the Jungle.txt\n",
      "Working with file : The Mockingbird.txt\n",
      "Working with file : Fat And Thin.txt\n",
      "Working with file : Vanka.txt\n",
      "Working with file : A Chameleon.txt\n",
      "Working with file : The Interlopers.txt\n",
      "Working with file : A Horseman in the Sky.txt\n",
      "Working with file : The Dead - James Joyce.txt\n",
      "Working with file : The McWilliamses And The Burglar Alarm.txt\n"
     ]
    }
   ],
   "source": [
    "for fl in onlyfiles:\n",
    "    try:\n",
    "        tmp_fle = ''\n",
    "\n",
    "        with open('/home/scarket/Project/Data Science/NLP/Short Story Analyzer/Resources/' + fl) as file:\n",
    "            tmp_fle = file.read()\n",
    "\n",
    "        print('Working with file : ' + fl)\n",
    "\n",
    "        tmp_fle = remove_punc(tmp_fle)\n",
    "\n",
    "        tmp_fle = lemmatizer(tmp_fle)\n",
    "\n",
    "        tmp_fle = nlp(tmp_fle)\n",
    "\n",
    "        tmp_fle = [str(word) for word in tmp_fle if word.is_stop == False]\n",
    "\n",
    "        tmp_fle = ' '.join(list(tmp_fle))\n",
    "\n",
    "        sl = pos_neg_stat(tmp_fle)\n",
    "\n",
    "    #         save_senty_count(fl + ' senty_count', sl)\n",
    "\n",
    "        with open(str(fl)+ ' senty_count'+'.json', 'w') as outfile:  \n",
    "            json.dump(sl, outfile)\n",
    "\n",
    "        sorted_count = Counter(tmp_fle.split()) # [token for token in cln_lemm.split()]\n",
    "\n",
    "        tmp = list(sorted_count.keys())\n",
    "\n",
    "        c_res = res_lst(tmp)\n",
    "\n",
    "    #         w_2_fle(fl+ 'res', c_res)\n",
    "\n",
    "        p_res = pos_count(c_res)\n",
    "\n",
    "        w_2_fle(fl+ 'pos_res', p_res)\n",
    "\n",
    "        with open(fl + ' res', 'w') as f:\n",
    "            for s in c_res:\n",
    "                f.write(str(s) + '\\n')\n",
    "\n",
    "        with open(fl + ' p res', 'w') as f:\n",
    "            for s in p_res:\n",
    "                f.write(str(s) + '\\n')\n",
    "\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Holiday Task.txt\n",
      "The Last Leaf.txt\n",
      "The Selfish Giant.txt\n",
      "A Lickpenny Lover.txt\n",
      "The Open Window.txt\n",
      "Upturned Face.txt\n",
      "The Necklace.txt\n",
      "The Hunter's Wife - ANTHONY DOERR.txt\n",
      "What Christmas Is As We Grow Older.txt\n",
      "The Bet.txt\n",
      "The Monkey's Paw - W. W. Jacobs.txt\n",
      "The Coming of the King.txt\n",
      "The Striding Place.txt\n",
      "The Cop and the Anthem.txt\n",
      "The Cask of Amontillado - Edgar Allan Poe.txt\n",
      "The Terrible Old Man.txt\n",
      "The Sphinx Without a Secret.txt\n",
      "The Thorny Road of Honor.txt\n",
      "A Respectable Woman.txt\n",
      "A Duel.txt\n",
      "How I Edited an Agricultural Paper.txt\n",
      "The Man In The Moon.txt\n",
      "The Nice People.txt\n",
      "A Defensive Diamond.txt\n",
      "The Tell-Tale Heart.txt\n",
      "Sredni Vashtar.txt\n",
      "The Veteran.txt\n",
      "The Disappearance of Crispina Umberleigh.txt\n",
      "A Cosmopolite in a Cafe.txt\n",
      "Federigo's Falcon.txt\n",
      "How the Widow Won the Deacon.txt\n",
      "The Merino Sheep.txt\n",
      "The Vendetta.txt\n",
      "The Little Match Girl.txt\n",
      "Jimmy Scarecrow's Christmas.txt\n",
      "Henry David Thoreau-A Child's Biography.txt\n",
      "Louisa May Alcott - A Child's Biography.txt\n",
      "Lost Hearts.txt\n",
      "A Defenseless Creature.txt\n",
      "The Log.txt\n",
      "Mark Twain - A Child's Biography.txt\n",
      "About Love.txt\n",
      "The Yarkand Manner.txt\n",
      "A Journey.txt\n",
      "Odour of Chrysanthemums.txt\n",
      "The Dreamer.txt\n",
      "The Cartographer Wasps and the Anarchist Bees - E. Lily Yu.txt\n",
      "Witches' Loaves.txt\n",
      "Ex Oblivione.txt\n",
      "The Colonel's Ideas.txt\n",
      "After the Race.txt\n",
      "The Veldt - Ray Bradbury.txt\n",
      "Boule de Suif.txt\n",
      "The Diary of a Madman.txt\n",
      "Symbols and Signs - Vladimir Nabokov.txt\n",
      "The Pit and the Pendulum.txt\n",
      "Hyacinth.txt\n",
      "Broads - Roxane Gay.txt\n",
      "The Death Of A Government Clerk.txt\n",
      "Jim Baker's Blue-Jay Yarn.txt\n",
      "Bertie's Christmas Eve.txt\n",
      "The Shoemaker And The Devil.txt\n",
      "Premium Harmony - STEVEN KING.txt\n",
      "The Way to the Dairy.txt\n",
      "The Unkindest Blow.txt\n",
      "Esme.txt\n",
      "The Luck of Roaring Camp.txt\n",
      "What You Want.txt\n",
      "The Water That Falls on You from Nowhere - John Chu.txt\n",
      "Bartleby, the Scrivener - Herman Melville.txt\n",
      "The Hanging Stranger.txt\n",
      "A True Story, Repeated Word for Word As I Heard It.txt\n",
      "The Cripple.txt\n",
      "The Nightingale and the Rose.txt\n",
      "A Dead Woman's Secret.txt\n",
      "The Game.txt\n",
      "Gabriel-Ernest.txt\n",
      "The Model Millionaire.txt\n",
      "The Man in the Brown Coat.txt\n",
      "Springtime a la Carte.txt\n",
      "The Furnished Room.txt\n",
      "Rikki-Tikki-Tavi.txt\n",
      "The Unrest-Cure.txt\n",
      "My Kinsman, Major Molineux.txt\n",
      "Hermann The Irascible.txt\n",
      "Gentle Hand.txt\n",
      "The Student.txt\n",
      "The Lumber Room.txt\n",
      "The Fly.txt\n",
      "The Ones Who Walk Away from Omelas - Ursula K. Le Guin.txt\n",
      "Hearts And Hands.txt\n",
      "A School Story.txt\n",
      "The Boarded Window.txt\n",
      "According to Their Lights.txt\n",
      "A Jury of Her Peers.txt\n",
      "YOUNGER WOMEN - KAREN JOY FOWLER.txt\n",
      "Alma - Junot DÐ±az.txt\n",
      "How the Leopard Got His Spots.txt\n",
      "The Celebrated Jumping Frog of Calaveras County - Mark Twain.txt\n",
      "The Story of Keesh.txt\n",
      "A Vine on a House.txt\n",
      "The Wolves of Cernogatz.txt\n",
      "The Cask of Amontillado.txt\n",
      "The Doll's House.txt\n",
      "A New England Nun.txt\n",
      "Transients in Arcadia.txt\n",
      "The Romance of a Busy Broker.txt\n",
      "A Father's Confession.txt\n",
      "The Princess And The Puma.txt\n",
      "Alexandre.txt\n",
      "One Summer Night.txt\n",
      "Turkeys Turning The Tables.txt\n",
      "The Night Came Slowly.txt\n",
      "A Retrieved Reformation.txt\n",
      "The Schartz-Metterklume Method.txt\n",
      "The Child's Story.txt\n",
      "Ali Baba and the Forty Thieves.txt\n",
      "On The Day of the Crucifixion.txt\n",
      "Hands.txt\n",
      "Aloha Oe.txt\n",
      "The Faery Handbag - Kelly Link.txt\n",
      "A Telephone Call.txt\n",
      "To Build a Fire.txt\n",
      "The Masque of the Red Death.txt\n",
      "The Last Fight In The Coliseum.txt\n",
      "An Alpine Divorce.txt\n",
      "The Father.txt\n",
      "The Affair at Coulter's Notch.txt\n",
      "Christmas Every Day.txt\n",
      "A Blunder.txt\n",
      "Kew Gardens.txt\n",
      "The Night Moth With a Crooked Feeler.txt\n",
      "Two Friends.txt\n",
      "Amy's Question.txt\n",
      "Chickamauga.txt\n",
      "The Hand.txt\n",
      "The Call of Cthulhu.txt\n",
      "A Baby Tramp.txt\n",
      "The Cats of Ulthar.txt\n",
      "The Huntsman.txt\n",
      "Eveline.txt\n",
      "Babes in the Jungle.txt\n",
      "The Repairer of Reputations.txt\n",
      "The Mockingbird.txt\n",
      "Fat And Thin.txt\n",
      "Vanka.txt\n",
      "The Aged Mother.txt\n",
      "A Chameleon.txt\n",
      "Ghosts and Empties - Lauren Groff.txt\n",
      "The Last Dream of Old Oak.txt\n",
      "The Interlopers.txt\n",
      "My Financial Career.txt\n",
      "A Horseman in the Sky.txt\n",
      "The Dead - James Joyce.txt\n",
      "The McWilliamses And The Burglar Alarm.txt\n"
     ]
    }
   ],
   "source": [
    "for fl in onlyfiles:\n",
    "    print(fl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
