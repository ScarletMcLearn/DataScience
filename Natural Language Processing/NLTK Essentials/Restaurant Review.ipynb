{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedback_simple = 'The schnitzel tastes good. The soup was too hot. The waiter was quick and polite.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The schnitzel tastes good\n",
      " The soup was too hot\n",
      " The waiter was quick and polite\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentences = feedback_simple.split('.')\n",
    "\n",
    "print('\\n'.join(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The\n",
      "schnitzel\n",
      "tastes\n",
      "good\n"
     ]
    }
   ],
   "source": [
    "words = sentences[0].split(' ')\n",
    "print('\\n'.join(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedback_rude = '''The waiter was very rude, \n",
    "e.g. when I accidentally opened the wrong door\n",
    "he screamed \"Private!\".'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The waiter was very rude, \n",
      "e\n",
      "g\n",
      " when I accidentally opened the wrong door\n",
      "he screamed \"Private!\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentences = feedback_rude.split('.')\n",
    "\n",
    "print('\\n'.join(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp_en = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The schnitzel tastes good.\n",
      "The soup was too hot.\n",
      "The waiter was quick and polite.\n"
     ]
    }
   ],
   "source": [
    "document = nlp_en(feedback_simple)\n",
    "\n",
    "for sentence in document.sents:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The waiter was very rude, \n",
      "e.g. when I accidentally opened the wrong door\n",
      "he screamed \"Private!\".\n"
     ]
    }
   ],
   "source": [
    "document_rude = nlp_en(feedback_rude)\n",
    "\n",
    "for sentence in document_rude.sents:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The\n",
      "schnitzel\n",
      "tastes\n",
      "good\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "first_sent = next(document.sents)\n",
    "\n",
    "for word in first_sent:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tastes\n"
     ]
    }
   ],
   "source": [
    "tastes_token = first_sent[2]\n",
    "\n",
    "print(tastes_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'taste'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tastes_token.lemma_  # basic form of word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'VERB'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tastes_token.pos_  # \"part of speech\" = role of word in sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VERB\n"
     ]
    }
   ],
   "source": [
    "print(tastes_token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tastes_token.pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99\n"
     ]
    }
   ],
   "source": [
    "from spacy.symbols import ADJ, NOUN, VERB\n",
    "\n",
    "print(VERB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91\n"
     ]
    }
   ],
   "source": [
    "print(NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VERB\n"
     ]
    }
   ],
   "source": [
    "from spacy.symbols import NAMES\n",
    "\n",
    "print(NAMES[99])  # 99 = VERB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99\n"
     ]
    }
   ],
   "source": [
    "from spacy.symbols import IDS\n",
    "\n",
    "print(IDS['VERB'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "class Topic(Enum):\n",
    "    AMBIENCE = 1\n",
    "    FOOD = 2\n",
    "    HYGIENE = 3\n",
    "    SERVICE = 4\n",
    "    VALUE = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rating(Enum):\n",
    "    VERY_BAD = -3\n",
    "    BAD = -2\n",
    "    SOMEWHAT_BAD = -1\n",
    "    SOMEWHAT_GOOD = 1\n",
    "    GOOD = 2\n",
    "    VERY_GOOD = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from spacy.tokens import Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LexiconEntry:\n",
    "    _IS_REGEX_REGEX = re.compile(r'.*[.+*\\[$^\\\\]')\n",
    "\n",
    "    def __init__(self, lemma: str, topic: Topic, rating: Rating):\n",
    "        assert lemma is not None\n",
    "        self.lemma = lemma\n",
    "        self._lower_lemma = lemma.lower()\n",
    "        self.topic = topic\n",
    "        self.rating = rating\n",
    "        self.is_regex = bool(LexiconEntry._IS_REGEX_REGEX.match(self.lemma))\n",
    "        self._regex = re.compile(lemma, re.IGNORECASE) if self.is_regex else None\n",
    "\n",
    "    def matching(self, token: Token) -> float:\n",
    "        \"\"\"\n",
    "        A weight between 0.0 and 1.0 on how much ``token`` matches this entry.\n",
    "        \"\"\"\n",
    "        assert token is not None\n",
    "        result = 0.0\n",
    "        if self.is_regex:\n",
    "            if self._regex.match(token.text):\n",
    "                result = 0.6\n",
    "            elif self._regex.match(token.lemma_):\n",
    "                result = 0.5\n",
    "        else:\n",
    "            if token.text == self.lemma:\n",
    "                result = 1.0\n",
    "            elif token.text.lower() == self.lemma:\n",
    "                result = 0.9\n",
    "            elif token.lemma_ == self.lemma:\n",
    "                result = 0.8\n",
    "            elif token.lemma_.lower() == self.lemma:\n",
    "                result = 0.7\n",
    "        return result\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        result = 'LexiconEntry(%s' % self.lemma\n",
    "        if self.topic is not None:\n",
    "            result += ', topic=%s' % self.topic.name\n",
    "        if self.rating is not None:\n",
    "            result += ', rating=%s' % self.rating.name\n",
    "        if self.is_regex:\n",
    "            result += ', is_regex=%s' % self.is_regex\n",
    "        result += ')'\n",
    "        return result\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return self.__str__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import isclose\n",
    "\n",
    "class Lexicon:\n",
    "    def __init__(self):\n",
    "#         List[LexiconEntry] = []\n",
    "        lst =  []\n",
    "        self.entries = lst\n",
    "\n",
    "    \n",
    "    def append(self, lemma: str, topic: Topic, rating: Rating):\n",
    "        lexicon_entry = LexiconEntry(lemma, topic, rating)\n",
    "        self.entries.append(lexicon_entry)\n",
    "\n",
    "    def lexicon_entry_for(self, token: Token) -> LexiconEntry:\n",
    "        \"\"\"\n",
    "        Entry in lexicon that best matches ``token``.\n",
    "        \"\"\"\n",
    "        result = None\n",
    "        lexicon_size = len(self.entries)\n",
    "        lexicon_entry_index = 0\n",
    "        best_matching = 0.0\n",
    "        while lexicon_entry_index < lexicon_size and not isclose(best_matching, 1.0):\n",
    "            lexicon_entry = self.entries[lexicon_entry_index]\n",
    "            matching = lexicon_entry.matching(token)\n",
    "            if matching > best_matching:\n",
    "                result = lexicon_entry\n",
    "                best_matching = matching\n",
    "            lexicon_entry_index += 1\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon = Lexicon()\n",
    "lexicon.append('waiter'     , Topic.SERVICE , None)\n",
    "lexicon.append('waitress'   , Topic.SERVICE , None)\n",
    "lexicon.append('wait'       , None          , Rating.BAD)\n",
    "lexicon.append('quick'      , None          , Rating.GOOD)\n",
    "lexicon.append('.*schnitzel', Topic.FOOD    , None)\n",
    "lexicon.append('music'      , Topic.AMBIENCE, None)\n",
    "lexicon.append('loud'       , None          , Rating.BAD)\n",
    "lexicon.append('tasty'      , Topic.FOOD    , Rating.GOOD)\n",
    "lexicon.append('polite'     , Topic.SERVICE , Rating.GOOD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The\n",
      "music\n",
      "was\n",
      "very\n",
      "loud\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "feedback_text = 'The music was very loud.'\n",
    "feedback = nlp_en(feedback_text)\n",
    "\n",
    "for token in next(feedback.sents):\n",
    "    lexicon_entry = lexicon.lexicon_entry_for(token)\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The music was very loud.\n",
      "     Topic.AMBIENCE\n",
      "     Rating.BAD\n"
     ]
    }
   ],
   "source": [
    "feedback_text = 'The music was very loud.'\n",
    "feedback = nlp_en(feedback_text)\n",
    "\n",
    "for sent in feedback.sents:\n",
    "    print(sent)\n",
    "    for token in sent:\n",
    "        lexicon_entry = lexicon.lexicon_entry_for(token)\n",
    "        if lexicon_entry is not None:\n",
    "            if lexicon_entry.topic is not None:\n",
    "                print('    ', lexicon_entry.topic)\n",
    "            if lexicon_entry.rating is not None:\n",
    "                print('    ', lexicon_entry.rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "INTENSIFIERS = {\n",
    "    'really',\n",
    "    'terribly',\n",
    "    'very',\n",
    "}\n",
    "\n",
    "def is_intensifier(token: Token) -> bool:\n",
    "    return token.lemma_.lower() in INTENSIFIERS\n",
    "\n",
    "DIMINISHERS = {\n",
    "    'barely',\n",
    "    'slightly',\n",
    "    'somewhat',\n",
    "}\n",
    "\n",
    "def is_diminisher(token: Token) -> bool:\n",
    "    return token.lemma_.lower() in DIMINISHERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "very\n"
     ]
    }
   ],
   "source": [
    "very_token = next(nlp_en(feedback_text).sents)[3]\n",
    "\n",
    "print(very_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_intensifier(very_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rating.SOMEWHAT_BAD\n",
      "Rating.SOMEWHAT_BAD\n",
      "Rating.VERY_BAD\n"
     ]
    }
   ],
   "source": [
    "def signum(value) -> int:\n",
    "    if value > 0:\n",
    "        return 1\n",
    "    elif value < 0:\n",
    "        return -1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "_MIN_RATING_VALUE = Rating.VERY_BAD.value\n",
    "_MAX_RATING_VALUE = Rating.VERY_GOOD.value\n",
    "\n",
    "\n",
    "def _ranged_rating(rating_value: int) -> Rating:\n",
    "    return Rating(min(_MAX_RATING_VALUE, max(_MIN_RATING_VALUE, rating_value)))\n",
    "\n",
    "def diminished(rating: Rating) -> Rating:\n",
    "    if abs(rating.value) > 1:\n",
    "        return _ranged_rating(rating.value - signum(rating.value))\n",
    "    else:\n",
    "        return rating\n",
    "\n",
    "def intensified(rating: Rating) -> Rating:\n",
    "    if abs(rating.value) > 1:\n",
    "        return _ranged_rating(rating.value + signum(rating.value))\n",
    "    else:\n",
    "        return rating\n",
    "\n",
    "print(diminished(Rating.BAD))\n",
    "print(diminished(Rating.SOMEWHAT_BAD))\n",
    "print(intensified(Rating.BAD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEGATIONS = {\n",
    "    'no',\n",
    "    'not',\n",
    "    'none',\n",
    "}\n",
    "\n",
    "def is_negation(token: Token) -> bool:\n",
    "    return token.lemma_.lower() in NEGATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rating.GOOD  ->  Rating.BAD\n",
      "Rating.VERY_BAD  ->  Rating.SOMEWHAT_GOOD\n"
     ]
    }
   ],
   "source": [
    "_RATING_TO_NEGATED_RATING_MAP = {\n",
    "    Rating.VERY_BAD     : Rating.SOMEWHAT_GOOD,\n",
    "    Rating.BAD          : Rating.GOOD,\n",
    "    Rating.SOMEWHAT_BAD : Rating.GOOD,  # hypothetical?\n",
    "    Rating.SOMEWHAT_GOOD: Rating.BAD,  # hypothetical?\n",
    "    Rating.GOOD         : Rating.BAD,\n",
    "    Rating.VERY_GOOD    : Rating.SOMEWHAT_BAD,\n",
    "}\n",
    "\n",
    "def negated_rating(rating: Rating) -> Rating:\n",
    "    assert rating is not None\n",
    "    return _RATING_TO_NEGATED_RATING_MAP[rating]\n",
    "\n",
    "print(Rating.GOOD, ' -> ', negated_rating(Rating.GOOD))\n",
    "print(Rating.VERY_BAD, ' -> ', negated_rating(Rating.VERY_BAD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "Token.set_extension('topic', default=None)\n",
    "Token.set_extension('rating', default=None)\n",
    "Token.set_extension('is_negation', default=False)\n",
    "Token.set_extension('is_intensifier', default=False)\n",
    "Token.set_extension('is_diminisher', default=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "schnitzel\n",
      "Topic.FOOD\n"
     ]
    }
   ],
   "source": [
    "token = next(nlp_en('schnitzel').sents)[0]\n",
    "print(token.lemma_)\n",
    "token._.topic = Topic.FOOD\n",
    "print(token._.topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token(schnitzel, lemma=schnitzel, topic=FOOD)\n"
     ]
    }
   ],
   "source": [
    "def debugged_token(token: Token) -> str:\n",
    "    result = 'Token(%s, lemma=%s' % (token.text, token.lemma_)\n",
    "    if token._.topic is not None:\n",
    "        result += ', topic=' + token._.topic.name\n",
    "    if token._.rating is not None:\n",
    "        result += ', rating=' + token._.rating.name\n",
    "    if token._.is_diminisher:\n",
    "        result += ', diminisher'\n",
    "    if token._.is_intensifier:\n",
    "        result += ', intensifier'\n",
    "    if token._.is_negation:\n",
    "        result += ', negation'\n",
    "    result += ')'\n",
    "    return result\n",
    "\n",
    "print(debugged_token(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def opinion_matcher(doc):\n",
    "    for sentence in doc.sents:\n",
    "        for token in sentence:\n",
    "            if is_intensifier(token):\n",
    "                token._.is_intensifier = True\n",
    "            elif is_diminisher(token):\n",
    "                token._.is_diminisher = True\n",
    "            elif is_negation(token):\n",
    "                token._.is_negation = True\n",
    "            else:\n",
    "                lexicon_entry = lexicon.lexicon_entry_for(token)\n",
    "                if lexicon_entry is not None:\n",
    "                    token._.rating = lexicon_entry.rating\n",
    "                    token._.topic = lexicon_entry.topic\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "if nlp_en.has_pipe('opinion_matcher'):\n",
    "    nlp_en.remove_pipe('opinion_matcher')\n",
    "nlp_en.add_pipe(opinion_matcher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_essential(token: Token) -> bool:\n",
    "    return token._.topic is not None \\\n",
    "        or token._.rating is not None \\\n",
    "        or token._.is_diminisher \\\n",
    "        or token._.is_intensifier \\\n",
    "        or token._.is_negation\n",
    "        \n",
    "def essential_tokens(tokens):\n",
    "    return [token for token in tokens if is_essential(token)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token(schnitzel, lemma=schnitzel, topic=FOOD)\n",
      "Token(not, lemma=not, negation)\n",
      "Token(very, lemma=very, intensifier)\n",
      "Token(tasty, lemma=tasty, topic=FOOD, rating=GOOD)\n"
     ]
    }
   ],
   "source": [
    "document = nlp_en('The schnitzel is not very tasty.')\n",
    "\n",
    "opinion_essence = essential_tokens(document)\n",
    "for token in opinion_essence:\n",
    "    print(debugged_token(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_rating_modifier(token: Token):\n",
    "    return token._.is_diminisher \\\n",
    "        or token._.is_intensifier \\\n",
    "        or token._.is_negation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_ratings(tokens):\n",
    "    # Find the first rating (if any).\n",
    "    rating_token_index = next(\n",
    "        (\n",
    "            token_index for token_index in range(len(tokens))\n",
    "            if tokens[token_index]._.rating is not None\n",
    "        ),\n",
    "        None  # Default if no rating token can be found\n",
    "        \n",
    "    )\n",
    "\n",
    "    if rating_token_index is not None:\n",
    "        # Apply modifiers to the left on the rating.\n",
    "        original_rating_token = tokens[rating_token_index]\n",
    "        combined_rating = original_rating_token._.rating\n",
    "        modifier_token_index = rating_token_index - 1\n",
    "        modified = True  # Did the last iteration modify anything?\n",
    "        while modified and modifier_token_index >= 0:\n",
    "            modifier_token = tokens[modifier_token_index]\n",
    "            if is_intensifier(modifier_token):\n",
    "                combined_rating = intensified(combined_rating)\n",
    "            elif is_diminisher(modifier_token):\n",
    "                combined_rating = diminished(combined_rating)\n",
    "            elif is_negation(modifier_token):\n",
    "                combined_rating = negated_rating(combined_rating)\n",
    "            else:\n",
    "                # We are done, no more modifiers \n",
    "                # to the left of this rating.\n",
    "                modified = False\n",
    "            if modified:\n",
    "                # Discord the current modifier \n",
    "                # and move on to the token on the left.\n",
    "                del tokens[modifier_token_index]\n",
    "                modifier_token_index -= 1\n",
    "        original_rating_token._.rating = combined_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "essential tokens:\n",
      "   Token(schnitzel, lemma=schnitzel, topic=FOOD)\n",
      "   Token(not, lemma=not, negation)\n",
      "   Token(very, lemma=very, intensifier)\n",
      "   Token(tasty, lemma=tasty, topic=FOOD, rating=GOOD)\n",
      "combined tokens:\n",
      "   Token(schnitzel, lemma=schnitzel, topic=FOOD)\n",
      "   Token(tasty, lemma=tasty, topic=FOOD, rating=SOMEWHAT_BAD)\n"
     ]
    }
   ],
   "source": [
    "document = nlp_en('The schnitzel is not very tasty.')\n",
    "\n",
    "opinion_essence = essential_tokens(document)\n",
    "print('essential tokens:')\n",
    "for token in opinion_essence:\n",
    "    print('  ', debugged_token(token))\n",
    "\n",
    "combine_ratings(opinion_essence)\n",
    "print('combined tokens:')\n",
    "for token in opinion_essence:\n",
    "    print('  ', debugged_token(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The schnitzel is not very tasty.\n",
      "(<Topic.FOOD: 2>, <Rating.SOMEWHAT_BAD: -1>)\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Tuple  # for fancy type hints\n",
    "\n",
    "def topic_and_rating_of(tokens: List[Token]) -> Tuple[Topic, Rating]:\n",
    "    result_topic = None\n",
    "    result_rating = None\n",
    "    opinion_essence = essential_tokens(tokens)\n",
    "    # print('  1: ', opinion_essence)\n",
    "    combine_ratings(opinion_essence)\n",
    "    # print('  2: ', opinion_essence)\n",
    "    for token in opinion_essence:\n",
    "        # print(debugged_token(token))\n",
    "        if (token._.topic is not None) and (result_topic is None):\n",
    "            result_topic = token._.topic\n",
    "        if (token._.rating is not None) and (result_rating is None):\n",
    "            result_rating = token._.rating\n",
    "        if (result_topic is not None) and (result_rating is not None):\n",
    "            break\n",
    "    return result_topic, result_rating\n",
    "\n",
    "sentence = next(nlp_en('The schnitzel is not very tasty.').sents)\n",
    "\n",
    "print(sentence)\n",
    "print(topic_and_rating_of(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def opinions(feedback_text: str):\n",
    "    feedback = nlp_en(feedback_text)\n",
    "    for tokens in feedback.sents:\n",
    "        yield(topic_and_rating_of(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic.FOOD Rating.SOMEWHAT_BAD\n",
      "Topic.SERVICE Rating.GOOD\n",
      "None None\n"
     ]
    }
   ],
   "source": [
    "feedback_text = \"\"\"\n",
    "The schnitzel was not very tasty. \n",
    "The waiter was polite.\n",
    "The football game ended 2:1.\"\"\"\n",
    "\n",
    "for topic, rating in opinions(feedback_text):\n",
    "    print(topic, rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
