{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " text wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('example.csv','rb') as f:\n",
    "    reader = csv.reader(f,delimiter=',',quotechar='\"')\n",
    "    for line in reader :\n",
    "        print line[1] # assuming the second field is the raw sting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "json data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "jsonfile = open('example.json')\n",
    "data = json.load(jsonfile)\n",
    "print data['string']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' This is an example sent.', 'The sentence splitter will split on sent markers.', 'Ohh really !', '!']\n"
     ]
    }
   ],
   "source": [
    "inputstring = ' This is an example sent. The sentence splitter will split on sent markers. Ohh really !!'\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "all_sent = sent_tokenize(inputstring)\n",
    "print (all_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.tokenize.punkt\n",
    "tokenizer = nltk.tokenize.punkt.PunktSentenceTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi', 'Everyone', '!', 'hola', 'gr8']\n"
     ]
    }
   ],
   "source": [
    "s = \"Hi Everyone ! hola gr8\" # simplest tokenizer\n",
    "print (s.split())\n",
    "# ['Hi', 'Everyone', '!', 'hola', 'gr8']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi', 'Everyone', '!', 'hola', 'gr8']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "word_tokenize(s)\n",
    "# ['Hi', 'Everyone', '!', 'hola', 'gr8']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi', 'Everyone', 'hola', 'gr8']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import regexp_tokenize, wordpunct_tokenize, blankline_tokenize\n",
    "\n",
    "regexp_tokenize(s, pattern='\\w+')\n",
    "# ['Hi', 'Everyone', 'hola', 'gr8']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['8']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regexp_tokenize(s, pattern='\\d+')\n",
    "# ['8']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi', 'Everyone', '!', 'hola', 'gr8']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordpunct_tokenize(s)\n",
    "# ['Hi', ',', 'Everyone', '!!', 'hola', 'gr8']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi Everyone ! hola gr8']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blankline_tokenize(s)\n",
    "# ['Hi, Everyone !! hola gr8']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer # import Porter stemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "pst = PorterStemmer() # create obj of the PorterStemmer\n",
    "lst = LancasterStemmer() # create obj of LancasterStemmer\n",
    "\n",
    "lst.stem(\"eating\")\n",
    "# eat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'shop'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pst.stem(\"shopping\")\n",
    "# shop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ate'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wlem = WordNetLemmatizer()\n",
    "wlem.lemmatize(\"ate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stopwords remover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stoplist = stopwords.words('english') # config the language name\n",
    "# NLTK supports 22 languages for removing the stop words\n",
    "text = \"This is just a test\"\n",
    "\n",
    "cleanwordlist = [word for word in text.split() if word not in\n",
    "stoplist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'test']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanwordlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rare word removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'token' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-8ff867962092>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# tokens is a list of all tokens in corpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfreq_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFreqDist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mrarewords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfreq_dist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mafter_rare_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrarewords\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'token' is not defined"
     ]
    }
   ],
   "source": [
    "# tokens is a list of all tokens in corpus\n",
    "freq_dist = nltk.FreqDist(token)\n",
    "rarewords = freq_dist.keys()[-50:]\n",
    "after_rare_words = [ word for word in token not in rarewords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spell correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.metrics import edit_distance\n",
    "\n",
    "edit_distance(\"rain\",\"shine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part of Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'PRP'), ('was', 'VBD'), ('watching', 'VBG'), ('TV', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "s = \"I was watching TV\"\n",
    "print( nltk.pos_tag(word_tokenize(s)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged = nltk.pos_tag(word_tokenize(s))\n",
    "allnoun = [word for word,pos in tagged if pos in ['NN','NNP'] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TV']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allnoun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stanford tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Can't instantiate abstract class StanfordTagger with abstract methods _cmd",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-3b12c9441b50>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mstan_tagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStanfordTagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'models/english-bidirectional-distdim.tagger'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'standford-postagger.jar'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Can't instantiate abstract class StanfordTagger with abstract methods _cmd"
     ]
    }
   ],
   "source": [
    "from nltk.tag.stanford import POSTagger\n",
    "import nltk\n",
    "\n",
    "stan_tagger = POSTagger('models/english-bidirectional-distdim.tagger','standford-postagger.jar')\n",
    "\n",
    "tokens = nltk.word_tokenize(s)\n",
    "stan_tagger.tag(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "deep diving tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 218 samples and 100554 outcomes>\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "import nltk\n",
    "\n",
    "tags = [tag for (word, tag) in brown.tagged_words(categories='news')]\n",
    "print (nltk.FreqDist(tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13089484257215028\n"
     ]
    }
   ],
   "source": [
    "brown_tagged_sents = brown.tagged_sents(categories='news')\n",
    "default_tagger = nltk.DefaultTagger('NN')\n",
    "print (default_tagger.evaluate(brown_tagged_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n-gram tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8363400777434467\n"
     ]
    }
   ],
   "source": [
    "from nltk.tag import UnigramTagger\n",
    "from nltk.tag import DefaultTagger\n",
    "from nltk.tag import BigramTagger\n",
    "from nltk.tag import TrigramTagger\n",
    "\n",
    "# we are dividing the data into a test and train to evaluate our taggers.\n",
    "train_data = brown_tagged_sents[:int(len(brown_tagged_sents) * 0.9)]\n",
    "test_data = brown_tagged_sents[int(len(brown_tagged_sents) * 0.9):]\n",
    "\n",
    "unigram_tagger = UnigramTagger(train_data,backoff=default_tagger)\n",
    "print( unigram_tagger.evaluate(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.84570915977275\n"
     ]
    }
   ],
   "source": [
    "bigram_tagger = BigramTagger(train_data, backoff=unigram_tagger)\n",
    "\n",
    "print( bigram_tagger.evaluate(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8442140934914781\n"
     ]
    }
   ],
   "source": [
    "trigram_tagger = TrigramTagger(train_data,backoff=bigram_tagger)\n",
    "\n",
    "print (trigram_tagger.evaluate(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regex tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.31306687929831556\n"
     ]
    }
   ],
   "source": [
    "from nltk.tag.sequential import RegexpTagger\n",
    "\n",
    "regexp_tagger = RegexpTagger(\n",
    " [( r'^-?[0-9]+(.[0-9]+)?$', 'CD'), # cardinal numbers\n",
    " ( r'(The|the|A|a|An|an)$', 'AT'), # articles\n",
    " ( r'.*able$', 'JJ'), # adjectives\n",
    " ( r'.*ness$', 'NN'), # nouns formed from adj\n",
    " ( r'.*ly$', 'RB'), # adverbs\n",
    " ( r'.*s$', 'NNS'), # plural nouns\n",
    " ( r'.*ing$', 'VBG'), # gerunds\n",
    " (r'.*ed$', 'VBD'), # past tense verbs\n",
    " (r'.*', 'NN') # nouns (default)\n",
    " ])\n",
    "\n",
    "print (regexp_tagger.evaluate(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NER tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON Mark/NNP)\n",
      "  is/VBZ\n",
      "  studying/VBG\n",
      "  at/IN\n",
      "  (ORGANIZATION Stanford/NNP University/NNP)\n",
      "  in/IN\n",
      "  (GPE California/NNP))\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import ne_chunk\n",
    "\n",
    "sent = \"Mark is studying at Stanford University in California\"\n",
    "print(ne_chunk(nltk.pos_tag(word_tokenize(sent)), binary=False))\n",
    "# (S\n",
    "#  (PERSON Mark/NNP)\n",
    "#  is/VBZ\n",
    "#  studying/VBG\n",
    "#  at/IN\n",
    "#  (ORGANIZATION Stanford/NNP University/NNP)\n",
    "#  in/IN\n",
    "#  NY(GPE California/NNP)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "unknown encoding: <PATH>/stanford-ner/stanford-ner.jar",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-f4b2e2f0cd2f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCoreNLPNERTagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'<PATH>/stanford-ner/classifiers/all.3class.distsim.crf.ser.gz'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'<PATH>/stanford-ner/stanford-ner.jar'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Rami Eid is studying at Stony Brook University in NY'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/share/virtualenvs/installers-OifosnB-/lib/python3.5/site-packages/nltk/tag/stanford.py\u001b[0m in \u001b[0;36mtag\u001b[0;34m(self, sentence)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag_sents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mraw_tag_sents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/installers-OifosnB-/lib/python3.5/site-packages/nltk/tag/stanford.py\u001b[0m in \u001b[0;36mtag_sents\u001b[0;34m(self, sentences)\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0;31m# Converting list(list(str)) -> list(str)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_tag_sents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/installers-OifosnB-/lib/python3.5/site-packages/nltk/tag/stanford.py\u001b[0m in \u001b[0;36mraw_tag_sents\u001b[0;34m(self, sentences)\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0mdefault_properties\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'annotators'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtagtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m             \u001b[0mtagged_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproperties\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault_properties\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtagged_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentences'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m             \u001b[0;31m# Taggers only need to return 1-best sentence.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/installers-OifosnB-/lib/python3.5/site-packages/nltk/parse/corenlp.py\u001b[0m in \u001b[0;36mapi_call\u001b[0;34m(self, data, properties)\u001b[0m\n\u001b[1;32m    245\u001b[0m                 \u001b[0;34m'properties'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_properties\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m             },\n\u001b[0;32m--> 247\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m             \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         )\n",
      "\u001b[0;31mLookupError\u001b[0m: unknown encoding: <PATH>/stanford-ner/stanford-ner.jar"
     ]
    }
   ],
   "source": [
    "from nltk.tag.stanford import CoreNLPNERTagger\n",
    "\n",
    "st = CoreNLPNERTagger('<PATH>/stanford-ner/classifiers/all.3class.distsim.crf.ser.gz', '<PATH>/stanford-ner/stanford-ner.jar')\n",
    "\n",
    "st.tag('Rami Eid is studying at Stony Brook University in NY'.split()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parsing Structure in Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:An unexpected error occurred while tokenizing input\n",
      "The following traceback may be corrupted or invalid\n",
      "The error message is: ('EOF in multi-line string', (1, 1))\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unable to parse line 2: S -> NP VP # S indicate the entire sentence\nExpected a nonterminal, found: # S indicate the entire sentence",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/.local/share/virtualenvs/installers-OifosnB-/lib/python3.5/site-packages/nltk/grammar.py\u001b[0m in \u001b[0;36mread_grammar\u001b[0;34m(input, nonterm_parser, probabilistic, encoding)\u001b[0m\n\u001b[1;32m   1286\u001b[0m                 \u001b[0;31m# expand out the disjunctions on the RHS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1287\u001b[0;31m                 \u001b[0mproductions\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0m_read_production\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnonterm_parser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobabilistic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1288\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/installers-OifosnB-/lib/python3.5/site-packages/nltk/grammar.py\u001b[0m in \u001b[0;36m_read_production\u001b[0;34m(line, nonterm_parser, probabilistic)\u001b[0m\n\u001b[1;32m   1229\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1230\u001b[0;31m             \u001b[0mnonterm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnonterm_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1231\u001b[0m             \u001b[0mrhsides\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnonterm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/installers-OifosnB-/lib/python3.5/site-packages/nltk/grammar.py\u001b[0m in \u001b[0;36mstandard_nonterm_parser\u001b[0;34m(string, pos)\u001b[0m\n\u001b[1;32m   1302\u001b[0m     if not m: raise ValueError('Expected a nonterminal, found: '\n\u001b[0;32m-> 1303\u001b[0;31m                                + string[pos:])\n\u001b[0m\u001b[1;32m   1304\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mNonterminal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected a nonterminal, found: # S indicate the entire sentence",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-f96562d893f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m  \u001b[0mDet\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"a\"\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;34m\"an\"\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;34m\"the\"\u001b[0m \u001b[0;31m# Det is determiner used in the sentences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m  \u001b[0mN\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"president\"\u001b[0m \u001b[0;34m|\u001b[0m\u001b[0;34m\"Obama\"\u001b[0m \u001b[0;34m|\u001b[0m\u001b[0;34m\"apple\"\u001b[0m\u001b[0;34m|\u001b[0m \u001b[0;34m\"coke\"\u001b[0m \u001b[0;31m# N some example nouns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m  \"\"\")\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mtoy_grammar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproductions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/installers-OifosnB-/lib/python3.5/site-packages/nltk/grammar.py\u001b[0m in \u001b[0;36mfromstring\u001b[0;34m(cls, input, encoding)\u001b[0m\n\u001b[1;32m    534\u001b[0m         \"\"\"\n\u001b[1;32m    535\u001b[0m         start, productions = read_grammar(input, standard_nonterm_parser,\n\u001b[0;32m--> 536\u001b[0;31m                                           encoding=encoding)\n\u001b[0m\u001b[1;32m    537\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mCFG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproductions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/installers-OifosnB-/lib/python3.5/site-packages/nltk/grammar.py\u001b[0m in \u001b[0;36mread_grammar\u001b[0;34m(input, nonterm_parser, probabilistic, encoding)\u001b[0m\n\u001b[1;32m   1288\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1289\u001b[0m             raise ValueError('Unable to parse line %s: %s\\n%s' %\n\u001b[0;32m-> 1290\u001b[0;31m                              (linenum+1, line, e))\n\u001b[0m\u001b[1;32m   1291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1292\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mproductions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unable to parse line 2: S -> NP VP # S indicate the entire sentence\nExpected a nonterminal, found: # S indicate the entire sentence"
     ]
    }
   ],
   "source": [
    "# toy CFG\n",
    "from nltk import CFG\n",
    "\n",
    "toy_grammar = nltk.CFG.fromstring(\n",
    "\"\"\"\n",
    " S -> NP VP # S indicate the entire sentence\n",
    " VP -> V NP # VP is verb phrase the\n",
    " V -> \"eats\" | \"drinks\" # V is verb\n",
    " NP -> Det N # NP is noun phrase (chunk that has noun in it)\n",
    " Det -> \"a\" | \"an\" | \"the\" # Det is determiner used in the sentences\n",
    " N -> \"president\" |\"Obama\" |\"apple\"| \"coke\" # N some example nouns\n",
    " \"\"\")\n",
    "\n",
    "toy_grammar.productions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A regex parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ChunkRule' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-6452c9e04bed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Regex parser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mchunk_rules\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mChunkRule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"<.*>+\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"chunk everything\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregexp\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ChunkRule' is not defined"
     ]
    }
   ],
   "source": [
    "# Regex parser\n",
    "chunk_rules=ChunkRule(\"<.*>+\",\"chunk everything\")\n",
    "\n",
    "import nltk\n",
    "from nltk.chunk.regexp import *\n",
    "reg_parser = RegexpParser('''\n",
    " NP: {<DT>? <JJ>* <NN>*} # NP\n",
    " P: {<IN>} # Preposition\n",
    " V: {<V.*>} # Verb\n",
    " PP: {<P> <NP>} # PP -> P NP\n",
    " VP: {<V> <NP|PP>*} # VP -> V (NP|PP)*\n",
    " ''')\n",
    "\n",
    "test_sent=\"Mr. Obama played a big role in the Health insurance bill\"\n",
    "test_sent_pos=nltk.pos_tag(nltk.word_tokenize(test_sent))\n",
    "paresed_out=reg_parser.parse(test_sent_pos)\n",
    "print (paresed_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Named-entity recognition (NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NP chunking (NER)\n",
    "# f=open(# absolute path for the file of text for which we want NER)\n",
    "# )\n",
    "# text=f.read()\n",
    "    \n",
    "sentences = nltk.sent_tokenize(b)\n",
    "tokenized_sentences = [nltk.word_tokenize(sentence) for sentence in\n",
    "sentences]\n",
    "tagged_sentences = [nltk.pos_tag(sentence) for sentence in tokenized_sentences]\n",
    "\n",
    "for sent in tagged_sentences:\n",
    "    print nltk.ne_chunk(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relation extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ORG: 'WHYY'] 'in' [LOC: 'Philadelphia']\n",
      "[ORG: 'McGlashan &AMP; Sarrail'] 'firm in' [LOC: 'San Mateo']\n",
      "[ORG: 'Freedom Forum'] 'in' [LOC: 'Arlington']\n",
      "[ORG: 'Brookings Institution'] ', the research group in' [LOC: 'Washington']\n",
      "[ORG: 'Idealab'] ', a self-described business incubator based in' [LOC: 'Los Angeles']\n",
      "[ORG: 'Open Text'] ', based in' [LOC: 'Waterloo']\n",
      "[ORG: 'WGBH'] 'in' [LOC: 'Boston']\n",
      "[ORG: 'Bastille Opera'] 'in' [LOC: 'Paris']\n",
      "[ORG: 'Omnicom'] 'in' [LOC: 'New York']\n",
      "[ORG: 'DDB Needham'] 'in' [LOC: 'New York']\n",
      "[ORG: 'Kaplan Thaler Group'] 'in' [LOC: 'New York']\n",
      "[ORG: 'BBDO South'] 'in' [LOC: 'Atlanta']\n",
      "[ORG: 'Georgia-Pacific'] 'in' [LOC: 'Atlanta']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "IN = re.compile(r'.*\\bin\\b(?!\\b.+ing)')\n",
    "\n",
    "for doc in nltk.corpus.ieer.parsed_docs('NYT_19980315'):\n",
    "     for rel in nltk.sem.extract_rels('ORG', 'LOC', doc, corpus='ieer', pattern = IN):\n",
    "\n",
    "        print(nltk.sem.rtuple(rel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLP Applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'nyt.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-8c67ec11db7f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'nyt.txt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mnews_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'nyt.txt'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "f=open('nyt.txt','r')\n",
    "news_content=f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "results=[]\n",
    "\n",
    "for sent_no,sentence in enumerate(nltk.sent_tokenize(news_content)):\n",
    "     no_of_tokens=len(nltk.word_tokenize(sentence))\n",
    "     #print no_of_toekns\n",
    "     # Let's do POS tagging\n",
    "     tagged=nltk.pos_tag(nltk.word_tokenize(sentence))\n",
    "     # Count the no of Nouns in the sentence\n",
    "     no_of_nouns=len([word for word,pos in tagged if pos in [\"NN\",\"NNP\"] ])\n",
    "     #Use NER to tag the named entities.\n",
    "     ners=nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sentence)),\n",
    "    binary=False)\n",
    "     no_of_ners= len([chunk for chunk in ners if hasattr(chunk,\n",
    "    'node')])\n",
    "     score=(no_of_ners+no_of_nouns)/float(no_of_toekns)\n",
    "\n",
    "     results.append((sent_no,no_of_tokens,no_of_ners,\\\n",
    "    no_of_nouns,score,sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in sorted(results,key=lambda x: x[4],reverse=True):\n",
    "\n",
    "print (sent[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-48-f6938cb31711>, line 25)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-48-f6938cb31711>\"\u001b[0;36m, line \u001b[0;32m25\u001b[0m\n\u001b[0;31m    results.append(i.sum()/float(len(i.nonzero()[0]))\u001b[0m\n\u001b[0m                                                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "results=[]\n",
    "\n",
    "news_content=\"\"\"Mr. Obama planned to promote the effort on Monday during\n",
    "a visit to Camden, N.J. The ban is part of Mr. Obama's push to ease\n",
    "tensions between law enforcement and minority \\communities in reaction to\n",
    "the crises in Baltimore; Ferguson, Mo. We are, without a doubt, sitting\n",
    "at a defining moment in American policing, Ronald L. Davis, the director\n",
    "of the Office of Community Oriented Policing Services at the Department\n",
    "of Justice, told reporters in a conference call organized by the White\n",
    "House\"\"\"\n",
    "\n",
    "sentences=nltk.sent_tokenize(news_content)\n",
    "\n",
    "vectorizer = TfidfVectorizer(norm='l2',min_df=0, use_idf=True, smooth_idf=False, sublinear_tf=True)\n",
    "\n",
    "sklearn_binary=vectorizer.fit_transform(sentences)\n",
    "print (countvectorizer.get_feature_names())\n",
    "\n",
    "print (sklearn_binary.toarray())\n",
    "\n",
    "for i in sklearn_binary.toarray():\n",
    "     results.append(i.sum()/float(len(i.nonzero()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import csv\n",
    "\n",
    "def preprocessing(text):\n",
    "     text = text.decode(\"utf8\")\n",
    "     # tokenize into words\n",
    "     tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "\n",
    "    # remove stopwords\n",
    "     stop = stopwords.words('english')\n",
    "     tokens = [token for token in tokens if token not in stop]\n",
    "     # remove words less than three letters\n",
    "     tokens = [word for word in tokens if len(word) >= 3]\n",
    "\n",
    "     # lower capitalization\n",
    "     tokens = [word.lower() for word in tokens]\n",
    "     # lemmatize\n",
    "     lmtzr = WordNetLemmatizer()\n",
    "     tokens = [lmtzr.lemmatize(word) for word in tokens]\n",
    "     preprocessed_text= ' '.join(tokens)\n",
    "     return preprocessed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>>smsdata = open('SMSSpamCollection') # check the structure of this\n",
    "file!\n",
    ">>>smsdata_data = []\n",
    ">>>sms_labels = []\n",
    ">>>csv_reader = csv.reader(sms,delimiter='\\t')\n",
    ">>>for line in csv_reader:\n",
    ">>> # adding the sms_id\n",
    ">>> sms_labels.append( line[0])\n",
    ">>> # adding the cleaned text We are calling preprocessing method\n",
    ">>> sms_data.append(preprocessing(line[1]))\n",
    ">>>sms.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>>import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>>trainset_size = int(round(len(sms_data)*0.70))\n",
    ">>># i chose this threshold for 70:30 train and test split.\n",
    ">>>print 'The training set size for this classifier is ' + str(trainset_\n",
    "size) + '\\n'\n",
    ">>>x_train = np.array([''.join(el) for el in sms_data[0:trainset_size]])\n",
    ">>>y_train = np.array([el for el in sms_labels[0:trainset_size]])\n",
    ">>>x_test = np.array([''.join(el) for el in sms_data[trainset_\n",
    "size+1:len(sms_data)]])\n",
    ">>>y_test = np.array([el for el in sms_labels[trainset_size+1:len(sms_\n",
    "labels)]])or el in sms_labels[trainset_size+1:len(sms_labels)]])\n",
    ">>>print x_train\n",
    ">>>print y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>>from sklearn.feature_extraction.text import CountVectorizer\n",
    ">>>sms_exp=[ ]\n",
    ">>>for line in sms_list:\n",
    ">>> sms_exp.append(preprocessing(line[1]))\n",
    ">>>vectorizer = CountVectorizer(min_df=1)\n",
    ">>>X_exp = vectorizer.fit_transform(sms_exp)\n",
    ">>>print \"||\".join(vectorizer.get_feature_names())\n",
    ">>>print X_exp.toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>>from sklearn.feature_extraction.text import TfidfVectorizer\n",
    ">>>vectorizer = TfidfVectorizer(min_df=2, ngram_range=(1, 2), stop_\n",
    "words='english', strip_accents='unicode', norm='l2')\n",
    ">>>X_train = vectorizer.fit_transform(x_train)\n",
    ">>>X_test = vectorizer.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>>from sklearn.naive_bayes import MultinomialNB\n",
    ">>>clf = MultinomialNB().fit(X_train, y_train)\n",
    ">>>y_nb_predicted = clf.predict(X_test)\n",
    ">>>print y_nb_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>>print ' \\n confusion_matrix \\n '\n",
    ">>>cm = confusion_matrix(y_test, y_pred)\n",
    ">>>print cm\n",
    ">>>print '\\n Here is the classification report:'\n",
    ">>>print classification_report(y_test, y_nb_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>>feature_names = vectorizer.get_feature_names()\n",
    ">>>coefs = clf.coef_\n",
    ">>>intercept = clf.intercept_\n",
    ">>>coefs_with_fns = sorted(zip(clf.coef_[0], feature_names))\n",
    ">>>n = 10\n",
    ">>>top = zip(coefs_with_fns[:n], coefs_with_fns[:-(n + 1):-1])\n",
    ">>>for (coef_1, fn_1), (coef_2, fn_2) in top:\n",
    ">>> print('\\t%.4f\\t%-15s\\t\\t%.4f\\t%-15s' % (coef_1, fn_1, coef_2,\n",
    "fn_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>>from sklearn import tree\n",
    ">>>clf = tree.DecisionTreeClassifier().fit(X_train.toarray(), y_train)\n",
    ">>>y_tree_predicted = clf.predict(X_test.toarray())\n",
    ">>>print y_tree_predicted\n",
    ">>>print ' \\n Here is the classification report:'\n",
    ">>>print classification_report(y_test, y_tree_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>>from sklearn.linear_model import SGDClassifier\n",
    ">>>from sklearn.metrics import confusion_matrix\n",
    ">>>clf = SGDClassifier(alpha=.0001, n_iter=50).fit(X_train, y_train)\n",
    ">>>y_pred = clf.predict(X_test)\n",
    ">>>print '\\n Here is the classification report:'\n",
    ">>>print classification_report(y_test, y_pred)\n",
    ">>>print ' \\n confusion_matrix \\n '\n",
    ">>>cm = confusion_matrix(y_test, y_pred)\n",
    ">>>print cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Random forest algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>>from sklearn.ensemble import RandomForestClassifier\n",
    ">>>RF_clf = RandomForestClassifier(n_estimators=10)\n",
    ">>>predicted = RF_clf.predict(X_test)\n",
    ">>>print '\\n Here is the classification report:'\n",
    ">>>print classification_report(y_test, predicted)\n",
    ">>>cm = confusion_matrix(y_test, y_pred)\n",
    ">>>print cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>>from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    ">>>true_k=5\n",
    ">>>km = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_\n",
    "init=1)\n",
    ">>>kmini = MiniBatchKMeans(n_clusters=true_k, init='k-means++', n_init=1,\n",
    "init_size=1000, batch_size=1000, verbose=opts.verbose)\n",
    ">>># we are using the same test,train data in TFIDF form as we did in\n",
    "text classification\n",
    ">>>km_model=km.fit(X_train)\n",
    ">>>kmini_model=kmini.fit(X_train)\n",
    ">>>print \"For K-mean clustering \"\n",
    ">>>clustering = collections.defaultdict(list)\n",
    ">>>for idx, label in enumerate(km_model.labels_):\n",
    ">>> clustering[label].append(idx)\n",
    ">>>print \"For K-mean Mini batch clustering \"\n",
    ">>>clustering = collections.defaultdict(list)\n",
    ">>>for idx, label in enumerate(kmini_model.labels_):\n",
    ">>> clustering[label].append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic modeling in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>>from gensim import corpora, models, similarities\n",
    ">>>from itertools import chain\n",
    ">>>import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>>from nltk.corpus import stopwords\n",
    ">>>from operator import itemgetter\n",
    ">>>import re\n",
    ">>>documents = [document for document in sms_data]\n",
    ">>>stoplist = stopwords.words('english')\n",
    ">>>texts = [[word for word in document.lower().split() if word not in\n",
    "stoplist] \\ for document in documents]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
