{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def remove_characters_before_tokenization(sentence,\n",
    "     keep_apostrophes=False):\n",
    "     sentence = sentence.strip()\n",
    "     if keep_apostrophes:\n",
    "         PATTERN = r'[?|$|&|*|%|@|(|)|~]' # add other characters here to remove them\n",
    "         filtered_sentence = re.sub(PATTERN, r'', sentence)\n",
    "     else:\n",
    "         PATTERN = r'[^a-zA-Z0-9 ]' # only extract alpha-numeric characters\n",
    "         filtered_sentence = re.sub(PATTERN, r'', sentence)\n",
    "     return filtered_sentence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Test sentence 1 2 3 4 '"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_characters_before_tokenization('Test sentence 1 2 3 4 @')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contractions import CONTRACTION_MAP\n",
    "\n",
    "def expand_contractions(sentence, contraction_mapping):\n",
    "     contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())),\n",
    "     flags=re.IGNORECASE|re.DOTALL)\n",
    "     def expand_match(contraction):\n",
    "         match = contraction.group(0)\n",
    "         first_char = match[0]\n",
    "         expanded_contraction = contraction_mapping.get(match)\\\n",
    "             if contraction_mapping.get(match)\\\n",
    "             else contraction_mapping.get(match.lower())\n",
    "         expanded_contraction = first_char+expanded_contraction[1:]\n",
    "         return expanded_contraction\n",
    "     expanded_sentence = contractions_pattern.sub(expand_match, sentence)\n",
    "     return expanded_sentence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a test sentence. But this is not one. has not she hearten?'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expand_contractions(\"This is a test sentence. But this isn't one. hasn't she hearten?\", CONTRACTION_MAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "     stopword_list = nltk.corpus.stopwords.words('english')\n",
    "     filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "     return filtered_tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test', 'sentence', '.', \"n't\", 'one', '.', \"n't\", 'hearten', '?']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_stopwords(nltk.word_tokenize(\"This is a test sentence. But this isn't one. hasn't she hearten?\".lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52.23\n",
      "12.5\n",
      "12.8\n",
      "11.61\n",
      "15.5\n",
      "7.49\n",
      "13\n",
      "13.833333333333334\n",
      "19.26146341463415\n",
      "12th and 13th grade\n"
     ]
    }
   ],
   "source": [
    "from textstat.textstat import textstat\n",
    "if __name__ == '__main__':\n",
    "        test_data = \"\"\"Playing games has always been thought to be important to the development of well-balanced and creative children; however, what part, if any, they should play in the lives of adults has never been researched that deeply. I believe that playing games is every bit as important for adults as for children. Not only is taking time out to play games with our children and other adults valuable to building interpersonal relationships but is also a wonderful way to release built up tension.\"\"\"\n",
    "\n",
    "        print(textstat.flesch_reading_ease(test_data))\n",
    "        print(textstat.smog_index(test_data))\n",
    "        print(textstat.flesch_kincaid_grade(test_data))\n",
    "        print(textstat.coleman_liau_index(test_data))\n",
    "        print(textstat.automated_readability_index(test_data))\n",
    "        print(textstat.dale_chall_readability_score(test_data))\n",
    "        print(textstat.difficult_words(test_data))\n",
    "        print(textstat.linsear_write_formula(test_data))\n",
    "        print(textstat.gunning_fog(test_data))\n",
    "        print(textstat.text_standard(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String\n",
      "it was the best of times it was the worst of times it was the age of wisdom it was the age of foolishness\n",
      "\n",
      "List\n",
      "['it', 'was', 'the', 'best', 'of', 'times', 'it', 'was', 'the', 'worst', 'of', 'times', 'it', 'was', 'the', 'age', 'of', 'wisdom', 'it', 'was', 'the', 'age', 'of', 'foolishness']\n",
      "\n",
      "Frequencies\n",
      "[4, 4, 4, 1, 4, 2, 4, 4, 4, 1, 4, 2, 4, 4, 4, 2, 4, 1, 4, 4, 4, 2, 4, 1]\n",
      "\n",
      "Pairs\n",
      "<zip object at 0x7fceba18b2c8>\n"
     ]
    }
   ],
   "source": [
    "# count-list-items-1.py\n",
    "\n",
    "wordstring = 'it was the best of times it was the worst of times '\n",
    "wordstring += 'it was the age of wisdom it was the age of foolishness'\n",
    "\n",
    "wordlist = wordstring.split()\n",
    "\n",
    "wordfreq = []\n",
    "for w in wordlist:\n",
    "    wordfreq.append(wordlist.count(w))\n",
    "\n",
    "print(\"String\\n\" + wordstring +\"\\n\")\n",
    "print(\"List\\n\" + str(wordlist) + \"\\n\")\n",
    "print(\"Frequencies\\n\" + str(wordfreq) + \"\\n\")\n",
    "print(\"Pairs\\n\" + str(zip(wordlist, wordfreq)))\n",
    "\n",
    "def word_freq_pair(wordstring):\n",
    "    wordlist = wordstring.split()\n",
    "    \n",
    "    wordfreq = []\n",
    "    \n",
    "    for w in wordlist:\n",
    "        wordfreq.append(wordlist.count(w))\n",
    "        \n",
    "    return list(zip(wordlist, wordfreq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('this', 1), ('is', 3), ('a', 1), ('test', 1), ('sentence', 1), ('is', 3), ('is', 3), ('is.', 1)]\n"
     ]
    }
   ],
   "source": [
    "print((word_freq_pair('this is a test sentence is is is.')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pairs\n",
      "[('it', 4), ('was', 4), ('the', 4), ('best', 1), ('of', 4), ('times', 2), ('it', 4), ('was', 4), ('the', 4), ('worst', 1), ('of', 4), ('times', 2), ('it', 4), ('was', 4), ('the', 4), ('age', 2), ('of', 4), ('wisdom', 1), ('it', 4), ('was', 4), ('the', 4), ('age', 2), ('of', 4), ('foolishness', 1)]\n"
     ]
    }
   ],
   "source": [
    "print(\"Pairs\\n\" + str(list(zip(wordlist, wordfreq))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a list of words, return a dictionary of\n",
    "# word-frequency pairs.\n",
    "\n",
    "def wordListToFreqDict(wordlist):\n",
    "    wordfreq = [wordlist.count(p) for p in wordlist]\n",
    "    return dict(zip(wordlist,wordfreq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort a dictionary of word-frequency pairs in\n",
    "# order of descending frequency.\n",
    "\n",
    "def sortFreqDict(freqdict):\n",
    "    aux = [(freqdict[key], key) for key in freqdict]\n",
    "    aux.sort()\n",
    "    aux.reverse()\n",
    "    return aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-aabb57e4c006>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msortFreqDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_freq_pair\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'this is a test sentence is is is.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-9b75deb0e094>\u001b[0m in \u001b[0;36msortFreqDict\u001b[0;34m(freqdict)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msortFreqDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfreqdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0maux\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfreqdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfreqdict\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0maux\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0maux\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreverse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-9b75deb0e094>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msortFreqDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfreqdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0maux\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfreqdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfreqdict\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0maux\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0maux\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreverse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "sortFreqDict((word_freq_pair('this is a test sentence is is is.')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "def removeStopwords(wordlist):\n",
    "    return [w for w in wordlist if w not in nltk.corpus.stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test', 'sentence.']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "removeStopwords(\"this is a test sentence.\".split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a list of n-grams identify the index of the keyword.\n",
    "import obo\n",
    "\n",
    "def nGramsToKWICDict(ngrams):\n",
    "    keyindex = len(ngrams[0]) // 2\n",
    "\n",
    "    return keyindex\n",
    "\n",
    "test = 'this test sentence has eight words in it'\n",
    "ngrams = obo.getNGrams(test.split(), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['this', 'test', 'sentence', 'has', 'eight'],\n",
       " ['test', 'sentence', 'has', 'eight', 'words'],\n",
       " ['sentence', 'has', 'eight', 'words', 'in'],\n",
       " ['has', 'eight', 'words', 'in', 'it']]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'words': [['has', 'eight', 'words', 'in', 'it']], 'sentence': [['this', 'test', 'sentence', 'has', 'eight']], 'has': [['test', 'sentence', 'has', 'eight', 'words']], 'eight': [['sentence', 'has', 'eight', 'words', 'in']]}\n"
     ]
    }
   ],
   "source": [
    "# Given a list of n-grams, return a dictionary of KWICs,\n",
    "# indexed by keyword.\n",
    "\n",
    "def nGramsToKWICDict(ngrams):\n",
    "    keyindex = len(ngrams[0]) // 2\n",
    "\n",
    "    kwicdict = {}\n",
    "\n",
    "    for k in ngrams:\n",
    "        if k[keyindex] not in kwicdict:\n",
    "            kwicdict[k[keyindex]] = [k]\n",
    "        else:\n",
    "            kwicdict[k[keyindex]].append(k)\n",
    "    return kwicdict\n",
    "\n",
    "print(nGramsToKWICDict(ngrams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a list of words and a number n, return a list\n",
    "# of n-grams.\n",
    "\n",
    "def getNGrams(wordlist, n):\n",
    "    return [wordlist[i:i+n] for i in range(len(wordlist)-(n-1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNGrams(wordlist, n):\n",
    "    ngrams = []\n",
    "    for i in range(len(wordlist)-(n-1)):\n",
    "        ngrams.append(wordlist[i:i+n])\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['it', 'was', 'the', 'best', 'of'], ['was', 'the', 'best', 'of', 'times'], ['the', 'best', 'of', 'times', 'it'], ['best', 'of', 'times', 'it', 'was'], ['of', 'times', 'it', 'was', 'the'], ['times', 'it', 'was', 'the', 'worst'], ['it', 'was', 'the', 'worst', 'of'], ['was', 'the', 'worst', 'of', 'times'], ['the', 'worst', 'of', 'times', 'it'], ['worst', 'of', 'times', 'it', 'was'], ['of', 'times', 'it', 'was', 'the'], ['times', 'it', 'was', 'the', 'age'], ['it', 'was', 'the', 'age', 'of'], ['was', 'the', 'age', 'of', 'wisdom'], ['the', 'age', 'of', 'wisdom', 'it'], ['age', 'of', 'wisdom', 'it', 'was'], ['of', 'wisdom', 'it', 'was', 'the'], ['wisdom', 'it', 'was', 'the', 'age'], ['it', 'was', 'the', 'age', 'of'], ['was', 'the', 'age', 'of', 'foolishness']]\n"
     ]
    }
   ],
   "source": [
    "#useGetNGrams.py\n",
    "\n",
    "import obo\n",
    "\n",
    "wordstring = 'it was the best of times it was the worst of times '\n",
    "wordstring += 'it was the age of wisdom it was the age of foolishness'\n",
    "allMyWords = wordstring.split()\n",
    "\n",
    "print(obo.getNGrams(allMyWords, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test1 = 'here are four words'\n",
    "test2 = 'this test sentence has eight words in it'\n",
    "\n",
    "getNGrams(test1.split(), 5)\n",
    "# -> []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['this', 'test', 'sentence', 'has', 'eight'],\n",
       " ['test', 'sentence', 'has', 'eight', 'words'],\n",
       " ['sentence', 'has', 'eight', 'words', 'in'],\n",
       " ['has', 'eight', 'words', 'in', 'it']]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getNGrams(test2.split(), 5)\n",
    "# -> [['this', 'test', 'sentence', 'has', 'eight'],\n",
    "# ['test', 'sentence', 'has', 'eight', 'words'],\n",
    "# ['sentence', 'has', 'eight', 'words', 'in'],\n",
    "# ['has', 'eight', 'words', 'in', 'it']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    " tweet='We have some delightful new food in the cafeteria. Awesome!!!'\n",
    " positive_words=['awesome','good', 'nice', 'super', 'fun', 'delightful']\n",
    " negative_words=['awful','lame','horrible','bad']\n",
    " words= tweet.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have some delightful new food in the cafeteria awesome\n",
      "We have some delightful new food in the cafeteria. Awesome!!!\n"
     ]
    }
   ],
   "source": [
    " from string import punctuation\n",
    " \n",
    " positive_counter=0\n",
    " tweet_processed=tweet.lower()\n",
    " for p in list(punctuation):\n",
    "     tweet_processed=tweet_processed.replace(p,'')\n",
    " \n",
    " print (tweet_processed)\n",
    " print(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to fix 4rm 7 - Twitter Bot - Testing Analysers - Ph01\n",
    "\n",
    "from string import punctuation\n",
    "\n",
    "postivie_words = ['delightful', 'awesome']\n",
    "\n",
    "def positivity(positive_words, text):\n",
    "    tweet_processed = text\n",
    "    positive_counter=0\n",
    "    \n",
    "    tweet_processed=tweet.lower()\n",
    "    \n",
    "    for p in list(punctuation):\n",
    "         tweet_processed=tweet_processed.replace(p,'')\n",
    "    \n",
    "    for word in tweet_processed.split(' '):\n",
    "         if word in positive_words:\n",
    "#              print (word+ ' is a positive word')\n",
    "             positive_counter=positive_counter+1\n",
    "    print(tweet_processed)\n",
    "\n",
    "    return positive_counter/len(tweet_processed)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03508771929824561"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_counter/len(tweet_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have some delightful new food in the cafeteria awesome\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.03508771929824561"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positivity(positive_words, 'this is! awesome')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " train = [\n",
      "     ('I love this sandwich.', 'pos'),\n",
      "     ('this is an amazing place!', 'pos'),\n",
      "     ('I feel very good about these beers.', 'pos'),\n",
      "     ('this is my best work.', 'pos'),\n",
      "     (\"what an awesome view\", 'pos'),\n",
      "     ('I do not like this restaurant', 'neg'),\n",
      "     ('I am tired of this stuff.', 'neg'),\n",
      "     (\"I can't deal with this\", 'neg'),\n",
      "     ('he is my sworn enemy!', 'neg'),\n",
      "     ('my boss is horrible.', 'neg')\n",
      " ]\n",
      " test = [\n",
      "     ('the beer was good.', 'pos'),\n",
      "     ('I do not enjoy my job', 'neg'),\n",
      "     (\"I ain't feeling dandy today.\", 'neg'),\n",
      "     (\"I feel amazing!\", 'pos'),\n",
      "     ('Gary is a friend of mine.', 'pos'),\n",
      "     (\"I can't believe I'm doing this.\", 'neg')\n",
      " ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "'''\n",
    ">>> train = [\n",
    "...     ('I love this sandwich.', 'pos'),\n",
    "...     ('this is an amazing place!', 'pos'),\n",
    "...     ('I feel very good about these beers.', 'pos'),\n",
    "...     ('this is my best work.', 'pos'),\n",
    "...     (\"what an awesome view\", 'pos'),\n",
    "...     ('I do not like this restaurant', 'neg'),\n",
    "...     ('I am tired of this stuff.', 'neg'),\n",
    "...     (\"I can't deal with this\", 'neg'),\n",
    "...     ('he is my sworn enemy!', 'neg'),\n",
    "...     ('my boss is horrible.', 'neg')\n",
    "... ]\n",
    ">>> test = [\n",
    "...     ('the beer was good.', 'pos'),\n",
    "...     ('I do not enjoy my job', 'neg'),\n",
    "...     (\"I ain't feeling dandy today.\", 'neg'),\n",
    "...     (\"I feel amazing!\", 'pos'),\n",
    "...     ('Gary is a friend of mine.', 'pos'),\n",
    "...     (\"I can't believe I'm doing this.\", 'neg')\n",
    "... ]\n",
    "'''.replace('>>>', '').replace('...', '')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = [\n",
    "     ('I love this sandwich.', 'pos'),\n",
    "     ('this is an amazing place!', 'pos'),\n",
    "     ('I feel very good about these beers.', 'pos'),\n",
    "     ('this is my best work.', 'pos'),\n",
    "     (\"what an awesome view\", 'pos'),\n",
    "     ('I do not like this restaurant', 'neg'),\n",
    "     ('I am tired of this stuff.', 'neg'),\n",
    "     (\"I can't deal with this\", 'neg'),\n",
    "     ('he is my sworn enemy!', 'neg'),\n",
    "     ('my boss is horrible.', 'neg')\n",
    " ]\n",
    " \n",
    "test = [\n",
    "     ('the beer was good.', 'pos'),\n",
    "     ('I do not enjoy my job', 'neg'),\n",
    "     (\"I ain't feeling dandy today.\", 'neg'),\n",
    "     (\"I feel amazing!\", 'pos'),\n",
    "     ('Gary is a friend of mine.', 'pos'),\n",
    "     (\"I can't believe I'm doing this.\", 'neg')\n",
    " ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob.classifiers import NaiveBayesClassifier\n",
    " \n",
    "cl = NaiveBayesClassifier(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pos'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cl.classify(\"This is an amazing library!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_dist = cl.prob_classify(\"This one's a doozy.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pos'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_dist.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.37"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(prob_dist.prob(\"neg\"), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8333333333333334"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " cl.accuracy(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "            contains(my) = True              neg : pos    =      1.7 : 1.0\n",
      "            contains(an) = False             neg : pos    =      1.6 : 1.0\n",
      "             contains(I) = True              neg : pos    =      1.4 : 1.0\n",
      "             contains(I) = False             pos : neg    =      1.4 : 1.0\n",
      "            contains(my) = False             pos : neg    =      1.3 : 1.0\n"
     ]
    }
   ],
   "source": [
    "cl.show_informative_features(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " new_data = [('She is my best friend.', 'pos'),\n",
      "            (\"I'm happy to have a new friend.\", 'pos'),\n",
      "            (\"Stay thirsty, my friend.\", 'pos'),\n",
      "            (\"He ain't from around here.\", 'neg')]\n",
      "cl.update(new_data)\n"
     ]
    }
   ],
   "source": [
    "print('''>>> new_data = [('She is my best friend.', 'pos'),\n",
    "...             (\"I'm happy to have a new friend.\", 'pos'),\n",
    "...             (\"Stay thirsty, my friend.\", 'pos'),\n",
    "...             (\"He ain't from around here.\", 'neg')]\n",
    ">>> cl.update(new_data)'''.replace('>>>', '').replace('...', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data = [('She is my best friend.', 'pos'),\n",
    "            (\"I'm happy to have a new friend.\", 'pos'),\n",
    "            (\"Stay thirsty, my friend.\", 'pos'),\n",
    "            (\"He ain't from around here.\", 'neg')]\n",
    "cl.update(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cl.accuracy(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "syns = wordnet.synsets(\"program\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plan.n.01\n"
     ]
    }
   ],
   "source": [
    "print(syns[0].name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plan\n"
     ]
    }
   ],
   "source": [
    "print(syns[0].lemmas()[0].name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a series of steps to be carried out or goals to be accomplished\n"
     ]
    }
   ],
   "source": [
    "print(syns[0].definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['they drew up a six-step plan', 'they discussed plans for a new bond issue']\n"
     ]
    }
   ],
   "source": [
    "print(syns[0].examples())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "they drew up a six-step plan\n"
     ]
    }
   ],
   "source": [
    "print(syns[0].examples()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('plan.n.01'), Synset('program.n.02'), Synset('broadcast.n.02'), Synset('platform.n.02'), Synset('program.n.05'), Synset('course_of_study.n.01'), Synset('program.n.07'), Synset('program.n.08'), Synset('program.v.01'), Synset('program.v.02')]\n"
     ]
    }
   ],
   "source": [
    "print(syns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'skillful', 'ripe', 'unspoilt', 'respectable', 'unspoiled', 'just', 'dependable', 'goodness', 'trade_good', 'dear', 'well', 'right', 'safe', 'estimable', 'sound', 'secure', 'adept', 'full', 'in_effect', 'soundly', 'near', 'serious', 'upright', 'honorable', 'in_force', 'commodity', 'thoroughly', 'expert', 'honest', 'effective', 'proficient', 'practiced', 'skilful', 'good', 'salutary', 'beneficial', 'undecomposed'}\n",
      "{'evilness', 'evil', 'badness', 'bad', 'ill'}\n"
     ]
    }
   ],
   "source": [
    "synonyms = []\n",
    "antonyms = []\n",
    "\n",
    "for syn in wordnet.synsets(\"good\"):\n",
    "    for l in syn.lemmas():\n",
    "        synonyms.append(l.name())\n",
    "        if l.antonyms():\n",
    "            antonyms.append(l.antonyms()[0].name())\n",
    "\n",
    "print(set(synonyms))\n",
    "print(set(antonyms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9090909090909091\n"
     ]
    }
   ],
   "source": [
    "w1 = wordnet.synset('ship.n.01')\n",
    "w2 = wordnet.synset('boat.n.01')\n",
    "print(w1.wup_similarity(w2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6956521739130435\n"
     ]
    }
   ],
   "source": [
    "w1 = wordnet.synset('ship.n.01')\n",
    "w2 = wordnet.synset('car.n.01')\n",
    "print(w1.wup_similarity(w2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.32\n"
     ]
    }
   ],
   "source": [
    "w1 = wordnet.synset('ship.n.01')\n",
    "w2 = wordnet.synset('cat.n.01')\n",
    "print(w1.wup_similarity(w2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('neg_word_list.pkl', 'rb') as f:\n",
    "    neg_word_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4555"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(neg_word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abnormal'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_word_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pos_word_list.pkl', 'rb') as f:\n",
    "    pos_word_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'able'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_word_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "syns = wordnet.synsets(neg_word_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abnormal powers of concentration', 'abnormal amounts of rain', 'abnormal circumstances', 'an abnormal interest in food']\n"
     ]
    }
   ],
   "source": [
    "print(syns[0].examples())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abnormal', 'powers', 'of', 'concentration']\n",
      "4\n",
      "['abnormal', 'amounts', 'of', 'rain']\n",
      "4\n",
      "['abnormal', 'circumstances']\n",
      "2\n",
      "['an', 'abnormal', 'interest', 'in', 'food']\n",
      "5\n",
      "an abnormal interest in food\n"
     ]
    }
   ],
   "source": [
    "maxi = 0\n",
    "maxi_item = \"\"\n",
    "\n",
    "for item in syns[0].examples():\n",
    "    if (len(word_tokenize(item))>maxi):\n",
    "        maxi = len(word_tokenize(item))\n",
    "        maxi_item = item\n",
    "    print(word_tokenize(item))\n",
    "    print(len(word_tokenize(item)))\n",
    "        \n",
    "print(maxi_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def get_example_sent(word):\n",
    "    maxi = 0\n",
    "    maxi_item = \"\"\n",
    "\n",
    "    syns = wordnet.synsets(word)\n",
    "    \n",
    "    for item in syns[0].examples():\n",
    "        if (len(word_tokenize(item))>maxi):\n",
    "            maxi = len(word_tokenize(item))\n",
    "            maxi_item = item\n",
    "#         print(word_tokenize(item))\n",
    "#         print(len(word_tokenize(item)))\n",
    "    if maxi_item == '':\n",
    "        maxi_item = word\n",
    "\n",
    "    return(maxi_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'are you going to sit on your fanny and do nothing?'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_example_sent('ass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I do it for the fun of it'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_example_sent('fun')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fuck'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_example_sent('fuck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [\n",
    "     ('I love this sandwich.', 'pos'),\n",
    "     ('this is an amazing place!', 'pos'),\n",
    "     ('I feel very good about these beers.', 'pos'),\n",
    "     ('this is my best work.', 'pos'),\n",
    "     (\"what an awesome view\", 'pos'),\n",
    "     ('I do not like this restaurant', 'neg'),\n",
    "     ('I am tired of this stuff.', 'neg'),\n",
    "     (\"I can't deal with this\", 'neg'),\n",
    "     ('he is my sworn enemy!', 'neg'),\n",
    "     ('my boss is horrible.', 'neg')\n",
    " ]\n",
    " \n",
    "pos_train = [\n",
    "     (get_example_sent('happy'), 'pos'),\n",
    "     (get_example_sent('fun'), 'pos'),\n",
    "     (get_example_sent('flower'), 'pos'),\n",
    "     (get_example_sent('cheerful'), 'pos'),\n",
    "     (get_example_sent('sweet'), 'pos'),\n",
    "     (get_example_sent('warm'), 'pos')\n",
    " ]\n",
    "\n",
    "neg_train = [\n",
    "     (get_example_sent('hurt'), 'neg'),\n",
    "     (get_example_sent('sad'), 'neg'),\n",
    "     (get_example_sent('depressed'), 'neg'),\n",
    "     (get_example_sent('kill'), 'neg'),\n",
    "     (get_example_sent('angry'), 'neg'),\n",
    "     (get_example_sent('dead'), 'neg')\n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spent many happy days on the beach', 'pos'),\n",
       " ('I do it for the fun of it', 'pos'),\n",
       " ('flower', 'pos'),\n",
       " ('as cheerful as anyone confined to a hospital bed could be', 'pos'),\n",
       " ('sweet', 'pos'),\n",
       " ('The soup warmed slowly on the stove', 'pos')]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hurt', 'neg'),\n",
       " ('Better by far that you should forget and smile / Than that you should remember and be sad\"- Christina Rossetti',\n",
       "  'neg'),\n",
       " (\"The bad state of her child's health demoralizes her\", 'neg'),\n",
       " ('kill', 'neg'),\n",
       " ('sending angry letters to the papers', 'neg'),\n",
       " ('they buried the dead', 'neg')]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = neg_train + pos_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob.classifiers import NaiveBayesClassifier\n",
    " \n",
    "cl = NaiveBayesClassifier(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'neg'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cl.classify(\"This is an sad!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_dist = cl.prob_classify(\"This is an sad!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.74"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(prob_dist.prob(\"neg\"), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_dist = cl.prob_classify(\"This is an sad hurt!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.91"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(prob_dist.prob(\"neg\"), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
