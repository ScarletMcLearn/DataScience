{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def remove_characters_before_tokenization(sentence,\n",
    "     keep_apostrophes=False):\n",
    "     sentence = sentence.strip()\n",
    "     if keep_apostrophes:\n",
    "         PATTERN = r'[?|$|&|*|%|@|(|)|~]' # add other characters here to remove them\n",
    "         filtered_sentence = re.sub(PATTERN, r'', sentence)\n",
    "     else:\n",
    "         PATTERN = r'[^a-zA-Z0-9 ]' # only extract alpha-numeric characters\n",
    "         filtered_sentence = re.sub(PATTERN, r'', sentence)\n",
    "     return filtered_sentence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Test sentence 1 2 3 4 '"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_characters_before_tokenization('Test sentence 1 2 3 4 @')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contractions import CONTRACTION_MAP\n",
    "\n",
    "def expand_contractions(sentence, contraction_mapping):\n",
    "     contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())),\n",
    "     flags=re.IGNORECASE|re.DOTALL)\n",
    "     def expand_match(contraction):\n",
    "         match = contraction.group(0)\n",
    "         first_char = match[0]\n",
    "         expanded_contraction = contraction_mapping.get(match)\\\n",
    "             if contraction_mapping.get(match)\\\n",
    "             else contraction_mapping.get(match.lower())\n",
    "         expanded_contraction = first_char+expanded_contraction[1:]\n",
    "         return expanded_contraction\n",
    "     expanded_sentence = contractions_pattern.sub(expand_match, sentence)\n",
    "     return expanded_sentence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a test sentence. But this is not one. has not she hearten?'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expand_contractions(\"This is a test sentence. But this isn't one. hasn't she hearten?\", CONTRACTION_MAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "     stopword_list = nltk.corpus.stopwords.words('english')\n",
    "     filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "     return filtered_tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test', 'sentence', '.', \"n't\", 'one', '.', \"n't\", 'hearten', '?']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_stopwords(nltk.word_tokenize(\"This is a test sentence. But this isn't one. hasn't she hearten?\".lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52.23\n",
      "12.5\n",
      "12.8\n",
      "11.61\n",
      "15.5\n",
      "7.49\n",
      "13\n",
      "13.833333333333334\n",
      "19.26146341463415\n",
      "12th and 13th grade\n"
     ]
    }
   ],
   "source": [
    "from textstat.textstat import textstat\n",
    "if __name__ == '__main__':\n",
    "        test_data = \"\"\"Playing games has always been thought to be important to the development of well-balanced and creative children; however, what part, if any, they should play in the lives of adults has never been researched that deeply. I believe that playing games is every bit as important for adults as for children. Not only is taking time out to play games with our children and other adults valuable to building interpersonal relationships but is also a wonderful way to release built up tension.\"\"\"\n",
    "\n",
    "        print(textstat.flesch_reading_ease(test_data))\n",
    "        print(textstat.smog_index(test_data))\n",
    "        print(textstat.flesch_kincaid_grade(test_data))\n",
    "        print(textstat.coleman_liau_index(test_data))\n",
    "        print(textstat.automated_readability_index(test_data))\n",
    "        print(textstat.dale_chall_readability_score(test_data))\n",
    "        print(textstat.difficult_words(test_data))\n",
    "        print(textstat.linsear_write_formula(test_data))\n",
    "        print(textstat.gunning_fog(test_data))\n",
    "        print(textstat.text_standard(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String\n",
      "it was the best of times it was the worst of times it was the age of wisdom it was the age of foolishness\n",
      "\n",
      "List\n",
      "['it', 'was', 'the', 'best', 'of', 'times', 'it', 'was', 'the', 'worst', 'of', 'times', 'it', 'was', 'the', 'age', 'of', 'wisdom', 'it', 'was', 'the', 'age', 'of', 'foolishness']\n",
      "\n",
      "Frequencies\n",
      "[4, 4, 4, 1, 4, 2, 4, 4, 4, 1, 4, 2, 4, 4, 4, 2, 4, 1, 4, 4, 4, 2, 4, 1]\n",
      "\n",
      "Pairs\n",
      "<zip object at 0x7fceba18b2c8>\n"
     ]
    }
   ],
   "source": [
    "# count-list-items-1.py\n",
    "\n",
    "wordstring = 'it was the best of times it was the worst of times '\n",
    "wordstring += 'it was the age of wisdom it was the age of foolishness'\n",
    "\n",
    "wordlist = wordstring.split()\n",
    "\n",
    "wordfreq = []\n",
    "for w in wordlist:\n",
    "    wordfreq.append(wordlist.count(w))\n",
    "\n",
    "print(\"String\\n\" + wordstring +\"\\n\")\n",
    "print(\"List\\n\" + str(wordlist) + \"\\n\")\n",
    "print(\"Frequencies\\n\" + str(wordfreq) + \"\\n\")\n",
    "print(\"Pairs\\n\" + str(zip(wordlist, wordfreq)))\n",
    "\n",
    "def word_freq_pair(wordstring):\n",
    "    wordlist = wordstring.split()\n",
    "    \n",
    "    wordfreq = []\n",
    "    \n",
    "    for w in wordlist:\n",
    "        wordfreq.append(wordlist.count(w))\n",
    "        \n",
    "    return list(zip(wordlist, wordfreq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('this', 1), ('is', 3), ('a', 1), ('test', 1), ('sentence', 1), ('is', 3), ('is', 3), ('is.', 1)]\n"
     ]
    }
   ],
   "source": [
    "print((word_freq_pair('this is a test sentence is is is.')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pairs\n",
      "[('it', 4), ('was', 4), ('the', 4), ('best', 1), ('of', 4), ('times', 2), ('it', 4), ('was', 4), ('the', 4), ('worst', 1), ('of', 4), ('times', 2), ('it', 4), ('was', 4), ('the', 4), ('age', 2), ('of', 4), ('wisdom', 1), ('it', 4), ('was', 4), ('the', 4), ('age', 2), ('of', 4), ('foolishness', 1)]\n"
     ]
    }
   ],
   "source": [
    "print(\"Pairs\\n\" + str(list(zip(wordlist, wordfreq))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a list of words, return a dictionary of\n",
    "# word-frequency pairs.\n",
    "\n",
    "def wordListToFreqDict(wordlist):\n",
    "    wordfreq = [wordlist.count(p) for p in wordlist]\n",
    "    return dict(zip(wordlist,wordfreq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort a dictionary of word-frequency pairs in\n",
    "# order of descending frequency.\n",
    "\n",
    "def sortFreqDict(freqdict):\n",
    "    aux = [(freqdict[key], key) for key in freqdict]\n",
    "    aux.sort()\n",
    "    aux.reverse()\n",
    "    return aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-aabb57e4c006>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msortFreqDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_freq_pair\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'this is a test sentence is is is.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-9b75deb0e094>\u001b[0m in \u001b[0;36msortFreqDict\u001b[0;34m(freqdict)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msortFreqDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfreqdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0maux\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfreqdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfreqdict\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0maux\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0maux\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreverse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-9b75deb0e094>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msortFreqDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfreqdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0maux\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfreqdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfreqdict\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0maux\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0maux\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreverse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "sortFreqDict((word_freq_pair('this is a test sentence is is is.')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "def removeStopwords(wordlist):\n",
    "    return [w for w in wordlist if w not in nltk.corpus.stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test', 'sentence.']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "removeStopwords(\"this is a test sentence.\".split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a list of n-grams identify the index of the keyword.\n",
    "import obo\n",
    "\n",
    "def nGramsToKWICDict(ngrams):\n",
    "    keyindex = len(ngrams[0]) // 2\n",
    "\n",
    "    return keyindex\n",
    "\n",
    "test = 'this test sentence has eight words in it'\n",
    "ngrams = obo.getNGrams(test.split(), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['this', 'test', 'sentence', 'has', 'eight'],\n",
       " ['test', 'sentence', 'has', 'eight', 'words'],\n",
       " ['sentence', 'has', 'eight', 'words', 'in'],\n",
       " ['has', 'eight', 'words', 'in', 'it']]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'words': [['has', 'eight', 'words', 'in', 'it']], 'sentence': [['this', 'test', 'sentence', 'has', 'eight']], 'has': [['test', 'sentence', 'has', 'eight', 'words']], 'eight': [['sentence', 'has', 'eight', 'words', 'in']]}\n"
     ]
    }
   ],
   "source": [
    "# Given a list of n-grams, return a dictionary of KWICs,\n",
    "# indexed by keyword.\n",
    "\n",
    "def nGramsToKWICDict(ngrams):\n",
    "    keyindex = len(ngrams[0]) // 2\n",
    "\n",
    "    kwicdict = {}\n",
    "\n",
    "    for k in ngrams:\n",
    "        if k[keyindex] not in kwicdict:\n",
    "            kwicdict[k[keyindex]] = [k]\n",
    "        else:\n",
    "            kwicdict[k[keyindex]].append(k)\n",
    "    return kwicdict\n",
    "\n",
    "print(nGramsToKWICDict(ngrams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a list of words and a number n, return a list\n",
    "# of n-grams.\n",
    "\n",
    "def getNGrams(wordlist, n):\n",
    "    return [wordlist[i:i+n] for i in range(len(wordlist)-(n-1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNGrams(wordlist, n):\n",
    "    ngrams = []\n",
    "    for i in range(len(wordlist)-(n-1)):\n",
    "        ngrams.append(wordlist[i:i+n])\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['it', 'was', 'the', 'best', 'of'], ['was', 'the', 'best', 'of', 'times'], ['the', 'best', 'of', 'times', 'it'], ['best', 'of', 'times', 'it', 'was'], ['of', 'times', 'it', 'was', 'the'], ['times', 'it', 'was', 'the', 'worst'], ['it', 'was', 'the', 'worst', 'of'], ['was', 'the', 'worst', 'of', 'times'], ['the', 'worst', 'of', 'times', 'it'], ['worst', 'of', 'times', 'it', 'was'], ['of', 'times', 'it', 'was', 'the'], ['times', 'it', 'was', 'the', 'age'], ['it', 'was', 'the', 'age', 'of'], ['was', 'the', 'age', 'of', 'wisdom'], ['the', 'age', 'of', 'wisdom', 'it'], ['age', 'of', 'wisdom', 'it', 'was'], ['of', 'wisdom', 'it', 'was', 'the'], ['wisdom', 'it', 'was', 'the', 'age'], ['it', 'was', 'the', 'age', 'of'], ['was', 'the', 'age', 'of', 'foolishness']]\n"
     ]
    }
   ],
   "source": [
    "#useGetNGrams.py\n",
    "\n",
    "import obo\n",
    "\n",
    "wordstring = 'it was the best of times it was the worst of times '\n",
    "wordstring += 'it was the age of wisdom it was the age of foolishness'\n",
    "allMyWords = wordstring.split()\n",
    "\n",
    "print(obo.getNGrams(allMyWords, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test1 = 'here are four words'\n",
    "test2 = 'this test sentence has eight words in it'\n",
    "\n",
    "getNGrams(test1.split(), 5)\n",
    "# -> []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['this', 'test', 'sentence', 'has', 'eight'],\n",
       " ['test', 'sentence', 'has', 'eight', 'words'],\n",
       " ['sentence', 'has', 'eight', 'words', 'in'],\n",
       " ['has', 'eight', 'words', 'in', 'it']]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getNGrams(test2.split(), 5)\n",
    "# -> [['this', 'test', 'sentence', 'has', 'eight'],\n",
    "# ['test', 'sentence', 'has', 'eight', 'words'],\n",
    "# ['sentence', 'has', 'eight', 'words', 'in'],\n",
    "# ['has', 'eight', 'words', 'in', 'it']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    " tweet='We have some delightful new food in the cafeteria. Awesome!!!'\n",
    " positive_words=['awesome','good', 'nice', 'super', 'fun', 'delightful']\n",
    " negative_words=['awful','lame','horrible','bad']\n",
    " words= tweet.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have some delightful new food in the cafeteria awesome\n",
      "We have some delightful new food in the cafeteria. Awesome!!!\n"
     ]
    }
   ],
   "source": [
    " from string import punctuation\n",
    " \n",
    " positive_counter=0\n",
    " tweet_processed=tweet.lower()\n",
    " for p in list(punctuation):\n",
    "     tweet_processed=tweet_processed.replace(p,'')\n",
    " \n",
    " print (tweet_processed)\n",
    " print(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to fix 4rm 7 - Twitter Bot - Testing Analysers - Ph01\n",
    "\n",
    "from string import punctuation\n",
    "\n",
    "postivie_words = ['delightful', 'awesome']\n",
    "\n",
    "def positivity(positive_words, text):\n",
    "    tweet_processed = text\n",
    "    positive_counter=0\n",
    "    \n",
    "    tweet_processed=tweet.lower()\n",
    "    \n",
    "    for p in list(punctuation):\n",
    "         tweet_processed=tweet_processed.replace(p,'')\n",
    "    \n",
    "    for word in tweet_processed.split(' '):\n",
    "         if word in positive_words:\n",
    "#              print (word+ ' is a positive word')\n",
    "             positive_counter=positive_counter+1\n",
    "    print(tweet_processed)\n",
    "\n",
    "    return positive_counter/len(tweet_processed)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03508771929824561"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_counter/len(tweet_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have some delightful new food in the cafeteria awesome\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.03508771929824561"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positivity(positive_words, 'this is! awesome')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
