{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52.23\n",
      "12.5\n",
      "12.8\n",
      "11.61\n",
      "15.5\n",
      "7.49\n",
      "13\n",
      "13.833333333333334\n",
      "19.26146341463415\n",
      "12th and 13th grade\n"
     ]
    }
   ],
   "source": [
    "from textstat.textstat import textstat\n",
    "if __name__ == '__main__':\n",
    "        test_data = \"\"\"Playing games has always been thought to be important to the development of well-balanced and creative children; however, what part, if any, they should play in the lives of adults has never been researched that deeply. I believe that playing games is every bit as important for adults as for children. Not only is taking time out to play games with our children and other adults valuable to building interpersonal relationships but is also a wonderful way to release built up tension.\"\"\"\n",
    "\n",
    "        print(textstat.flesch_reading_ease(test_data))\n",
    "        print(textstat.smog_index(test_data))\n",
    "        print(textstat.flesch_kincaid_grade(test_data))\n",
    "        print(textstat.coleman_liau_index(test_data))\n",
    "        print(textstat.automated_readability_index(test_data))\n",
    "        print(textstat.dale_chall_readability_score(test_data))\n",
    "        print(textstat.difficult_words(test_data))\n",
    "        print(textstat.linsear_write_formula(test_data))\n",
    "        print(textstat.gunning_fog(test_data))\n",
    "        print(textstat.text_standard(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flesch–Kincaid readability tests\n",
    "From Wikipedia, the free encyclopedia\n",
    "The Flesch–Kincaid readability tests are readability tests designed to indicate how difficult a passage in English is to understand. There are two tests, the Flesch Reading Ease, and the Flesch–Kincaid Grade Level. Although they use the same core measures (word length and sentence length), they have different weighting factors.\n",
    "\n",
    "The results of the two tests correlate approximately inversely: a text with a comparatively high score on the Reading Ease test should have a lower score on the Grade-Level test. Rudolf Flesch devised the Reading Ease evaluation; somewhat later, he and J. Peter Kincaid developed the Grade Level evaluation for the United States Navy.\n",
    "\n",
    "Contents \n",
    "1\tHistory\n",
    "2\tFlesch reading ease\n",
    "3\tFlesch–Kincaid grade level\n",
    "4\tApplications\n",
    "5\tSee also\n",
    "6\tReferences\n",
    "6.1\tFurther reading\n",
    "7\tExternal links\n",
    "History\n",
    "\"The Flesch–Kincaid\" (F–K) reading grade level was developed under contract to the U.S. Navy in 1975 by J. Peter Kincaid and his team.[1] Related U.S. Navy research directed by Kincaid delved into high-tech education (for example, the electronic authoring and delivery of technical information),[2] usefulness of the Flesch–Kincaid readability formula,[3] computer aids for editing tests,[4] illustrated formats to teach procedures,[5] and the Computer Readability Editing System (CRES).[6]\n",
    "\n",
    "The F–K formula was first used by the Army for assessing the difficulty of technical manuals in 1978 and soon after became a United States Military Standard. Pennsylvania was the first U.S. state to require that automobile insurance policies be written at no higher than a ninth-grade level (14–15 years of age) of reading difficulty, as measured by the F–K formula. This is now a common requirement in many other states and for other legal documents such as insurance policies.[3]\n",
    "\n",
    "Flesch reading ease\n",
    "In the Flesch reading-ease test, higher scores indicate material that is easier to read; lower numbers mark passages that are more difficult to read. The formula for the Flesch reading-ease score (FRES) test is\n",
    "\n",
    "{\\displaystyle 206.835-1.015\\left({\\frac {\\text{total words}}{\\text{total sentences}}}\\right)-84.6\\left({\\frac {\\text{total syllables}}{\\text{total words}}}\\right)} {\\displaystyle 206.835-1.015\\left({\\frac {\\text{total words}}{\\text{total sentences}}}\\right)-84.6\\left({\\frac {\\text{total syllables}}{\\text{total words}}}\\right)}[7]\n",
    "Scores can be interpreted as shown in the table below.[8]\n",
    "\n",
    "Score\tSchool level\tNotes\n",
    "100.00-90.00\t5th grade\tVery easy to read. Easily understood by an average 11-year-old student.\n",
    "90.0–80.0\t6th grade\tEasy to read. Conversational English for consumers.\n",
    "80.0–70.0\t7th grade\tFairly easy to read.\n",
    "70.0–60.0\t8th & 9th grade\tPlain English. Easily understood by 13- to 15-year-old students.\n",
    "60.0–50.0\t10th to 12th grade\tFairly difficult to read.\n",
    "50.0–30.0\tCollege\tDifficult to read.\n",
    "30.0–0.0\tCollege graduate\tVery difficult to read. Best understood by university graduates.\n",
    "Reader's Digest magazine has a readability index of about 65, Time magazine scores about 52, an average grade six student's written assignment (age of 12) has a readability index of 60–70 (and a reading grade level of six to seven), and the Harvard Law Review has a general readability score in the low 30s. The highest (easiest) readability score possible is 121.22, (i.e. every sentence consisting of only one one-syllable words). \"The cat sat on the mat.\" scores 116. The score does not have a theoretical lower bound, therefore it is possible to make the score as low as wanted by arbitrarily including words with many syllables. The sentence \"This sentence, taken as a reading passage unto itself, is being used to prove a point.\" has a readability of 74.1. The sentence \"The Australian platypus is seemingly a hybrid of a mammal and reptilian creature.\" scores 37.5 as it has 24 syllables and 13 words. While Amazon calculates the text of Moby Dick as 57.9,[9] one particularly long sentence about sharks in chapter 64 has a readability score of −146.77.[10] One sentence in the beginning of \"Swann's Way\", by Marcel Proust, has a score of −515.1.[11]\n",
    "\n",
    "The U.S. Department of Defense uses the reading ease test as the standard test of readability for its documents and forms.[12] Florida requires that life insurance policies have a Flesch reading ease score of 45 or greater.[13]\n",
    "\n",
    "Use of this scale is so ubiquitous that it is bundled with popular word processing programs and services such as KWord, IBM Lotus Symphony, Microsoft Office Word, WordPerfect, and WordPro.\n",
    "\n",
    "Polysyllabic words affect this score significantly more than they do the grade level score.\n",
    "\n",
    "Flesch–Kincaid grade level\n",
    "These readability tests are used extensively in the field of education. The \"Flesch–Kincaid Grade Level Formula\" instead presents a score as a U.S. grade level, making it easier for teachers, parents, librarians, and others to judge the readability level of various books and texts. It can also mean the number of years of education generally required to understand this text, relevant when the formula results in a number greater than 10. The grade level is calculated with the following formula:\n",
    "\n",
    "{\\displaystyle 0.39\\left({\\frac {\\mbox{total words}}{\\mbox{total sentences}}}\\right)+11.8\\left({\\frac {\\mbox{total syllables}}{\\mbox{total words}}}\\right)-15.59} 0.39\\left({\\frac  {{\\mbox{total words}}}{{\\mbox{total sentences}}}}\\right)+11.8\\left({\\frac  {{\\mbox{total syllables}}}{{\\mbox{total words}}}}\\right)-15.59[14]\n",
    "The result is a number that corresponds with a U.S. grade level. The sentence, \"The Australian platypus is seemingly a hybrid of a mammal and reptilian creature\" is an 11.3 as it has 24 syllables and 13 words. The different weighting factors for words per sentence and syllables per word in each scoring system mean that the two schemes are not directly comparable and cannot be converted. The grade level formula emphasises sentence length over word length. By creating one-word strings with hundreds of random characters, grade levels may be attained that are hundreds of times larger than high school completion in the United States. Due to the formula's construction, the score does not have an upper bound.\n",
    "\n",
    "The lowest grade level score in theory is −3.40, but there are few real passages in which every sentence consists of a single one-syllable word. Green Eggs and Ham by Dr. Seuss comes close, averaging 5.7 words per sentence and 1.02 syllables per word, with a grade level of −1.3. (Most of the 50 used words are monosyllabic; \"anywhere\", which occurs eight times, is the only exception.)\n",
    "\n",
    "Applications\n",
    "\n",
    "This section does not cite any sources. Please help improve this section by adding citations to reliable sources. Unsourced material may be challenged and removed. (August 2017) (Learn how and when to remove this template message)\n",
    "Leah Borovoi from the Infinity Labs[clarification needed] has calculated the Flesch score for the seven Harry Potter books that were located at the Glozman Website. The average Flesch score for Harry Potter was 72.83, with the highest score (81.32) for Harry Potter and the Philosopher's Stone and the lowest score (65.88) for Harry Potter and the Order of the Phoenix.\n",
    "\n",
    "Leah Borovoi also calculated the Flesch score for 2000 articles about people on Wikipedia. As you may see in the chart below, the most readable articles are by sportspeople and entertainers (actors and actresses), while the least readable articles are by scientists and philosophers. The least readable scientists are economists (Flesch score = 41.70), psychologists (42.25), chemists (42.81), and mathematicians (43.35)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = 'Blog Care* is independent of statutory agencies, and sees its self as essentially concerned with forging partnerships in order to facilitate the highest possible quality of life for older people and those in need of community support.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.0\n",
      "0\n",
      "20.1\n",
      "14.34\n",
      "22.3\n",
      "11.02\n",
      "13\n",
      "20.5\n",
      "30.854054054054053\n",
      "20th and 21th grade\n"
     ]
    }
   ],
   "source": [
    "        print(textstat.flesch_reading_ease(test_data))\n",
    "        print(textstat.smog_index(test_data))\n",
    "        print(textstat.flesch_kincaid_grade(test_data))\n",
    "        print(textstat.coleman_liau_index(test_data))\n",
    "        print(textstat.automated_readability_index(test_data))\n",
    "        print(textstat.dale_chall_readability_score(test_data))\n",
    "        print(textstat.difficult_words(test_data))\n",
    "        print(textstat.linsear_write_formula(test_data))\n",
    "        print(textstat.gunning_fog(test_data))\n",
    "        print(textstat.text_standard(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = '''Blog Care is a private company. We work in partnership with key agencies like the social services and Primary Care Trusts (GPs, social workers etc.) We aim to give the highest possible quality of life for older people and other adults in the community who need support.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64.0\n",
      "11.2\n",
      "8.2\n",
      "9.92\n",
      "8.7\n",
      "8.78\n",
      "13\n",
      "6.333333333333333\n",
      "19.343829787234046\n",
      "8th and 9th grade\n"
     ]
    }
   ],
   "source": [
    "        print(textstat.flesch_reading_ease(test_data))\n",
    "        print(textstat.smog_index(test_data))\n",
    "        print(textstat.flesch_kincaid_grade(test_data))\n",
    "        print(textstat.coleman_liau_index(test_data))\n",
    "        print(textstat.automated_readability_index(test_data))\n",
    "        print(textstat.dale_chall_readability_score(test_data))\n",
    "        print(textstat.difficult_words(test_data))\n",
    "        print(textstat.linsear_write_formula(test_data))\n",
    "        print(textstat.gunning_fog(test_data))\n",
    "        print(textstat.text_standard(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = 'The care home also has a purpose built restaurant that can be used by family and friends when they are visiting the home, a hairdressing salon and a sixty five place day care facility which offers a stimulating environment with a variety of activities to choose from.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.77\n",
      "0\n",
      "21.6\n",
      "10.87\n",
      "24.2\n",
      "9.66\n",
      "11\n",
      "25.5\n",
      "30.16170212765958\n",
      "10th and 11th grade\n"
     ]
    }
   ],
   "source": [
    "        print(textstat.flesch_reading_ease(test_data))\n",
    "        print(textstat.smog_index(test_data))\n",
    "        print(textstat.flesch_kincaid_grade(test_data))\n",
    "        print(textstat.coleman_liau_index(test_data))\n",
    "        print(textstat.automated_readability_index(test_data))\n",
    "        print(textstat.dale_chall_readability_score(test_data))\n",
    "        print(textstat.difficult_words(test_data))\n",
    "        print(textstat.linsear_write_formula(test_data))\n",
    "        print(textstat.gunning_fog(test_data))\n",
    "        print(textstat.text_standard(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = 'Sunset Hamlet has a purpose built restaurant that can be used by family and friends when they are visiting the home. It also boasts an excellent hairdressing salon. Our sixty five-place day care centre offers a wide range of stimulating activities to suit all service users.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64.41\n",
      "11.2\n",
      "8.1\n",
      "11.01\n",
      "9.7\n",
      "8.86\n",
      "13\n",
      "6.666666666666667\n",
      "19.424347826086958\n",
      "8th and 9th grade\n"
     ]
    }
   ],
   "source": [
    "        print(textstat.flesch_reading_ease(test_data))\n",
    "        print(textstat.smog_index(test_data))\n",
    "        print(textstat.flesch_kincaid_grade(test_data))\n",
    "        print(textstat.coleman_liau_index(test_data))\n",
    "        print(textstat.automated_readability_index(test_data))\n",
    "        print(textstat.dale_chall_readability_score(test_data))\n",
    "        print(textstat.difficult_words(test_data))\n",
    "        print(textstat.linsear_write_formula(test_data))\n",
    "        print(textstat.gunning_fog(test_data))\n",
    "        print(textstat.text_standard(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = '''I'm hungry. Feed me.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92.8\n",
      "0\n",
      "1.3\n",
      "1.45\n",
      "0.6\n",
      "7.78\n",
      "1\n",
      "1.0\n",
      "13.600000000000001\n",
      "0th and 1th grade\n"
     ]
    }
   ],
   "source": [
    "        print(textstat.flesch_reading_ease(test_data))\n",
    "        print(textstat.smog_index(test_data))\n",
    "        print(textstat.flesch_kincaid_grade(test_data))\n",
    "        print(textstat.coleman_liau_index(test_data))\n",
    "        print(textstat.automated_readability_index(test_data))\n",
    "        print(textstat.dale_chall_readability_score(test_data))\n",
    "        print(textstat.difficult_words(test_data))\n",
    "        print(textstat.linsear_write_formula(test_data))\n",
    "        print(textstat.gunning_fog(test_data))\n",
    "        print(textstat.text_standard(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COUNTING WORD FREQUENCIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String\n",
      "it was the best of times it was the worst of times it was the age of wisdom it was the age of foolishness\n",
      "\n",
      "List\n",
      "['it', 'was', 'the', 'best', 'of', 'times', 'it', 'was', 'the', 'worst', 'of', 'times', 'it', 'was', 'the', 'age', 'of', 'wisdom', 'it', 'was', 'the', 'age', 'of', 'foolishness']\n",
      "\n",
      "Frequencies\n",
      "[4, 4, 4, 1, 4, 2, 4, 4, 4, 1, 4, 2, 4, 4, 4, 2, 4, 1, 4, 4, 4, 2, 4, 1]\n",
      "\n",
      "Pairs\n",
      "<zip object at 0x7fef901a6b08>\n"
     ]
    }
   ],
   "source": [
    "# count-list-items-1.py\n",
    "\n",
    "wordstring = 'it was the best of times it was the worst of times '\n",
    "wordstring += 'it was the age of wisdom it was the age of foolishness'\n",
    "\n",
    "wordlist = wordstring.split()\n",
    "\n",
    "wordfreq = []\n",
    "for w in wordlist:\n",
    "    wordfreq.append(wordlist.count(w))\n",
    "\n",
    "print(\"String\\n\" + wordstring +\"\\n\")\n",
    "print(\"List\\n\" + str(wordlist) + \"\\n\")\n",
    "print(\"Frequencies\\n\" + str(wordfreq) + \"\\n\")\n",
    "print(\"Pairs\\n\" + str(zip(wordlist, wordfreq)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on zip object:\n",
      "\n",
      "class zip(object)\n",
      " |  zip(iter1 [,iter2 [...]]) --> zip object\n",
      " |  \n",
      " |  Return a zip object whose .__next__() method returns a tuple where\n",
      " |  the i-th element comes from the i-th iterable argument.  The .__next__()\n",
      " |  method continues until the shortest iterable in the argument sequence\n",
      " |  is exhausted and then it raises StopIteration.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __getattribute__(self, name, /)\n",
      " |      Return getattr(self, name).\n",
      " |  \n",
      " |  __iter__(self, /)\n",
      " |      Implement iter(self).\n",
      " |  \n",
      " |  __new__(*args, **kwargs) from builtins.type\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      " |  \n",
      " |  __next__(self, /)\n",
      " |      Implement next(self).\n",
      " |  \n",
      " |  __reduce__(...)\n",
      " |      Return state information for pickling.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(zip(wordlist, wordfreq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('it', 4),\n",
       " ('was', 4),\n",
       " ('the', 4),\n",
       " ('best', 1),\n",
       " ('of', 4),\n",
       " ('times', 2),\n",
       " ('it', 4),\n",
       " ('was', 4),\n",
       " ('the', 4),\n",
       " ('worst', 1),\n",
       " ('of', 4),\n",
       " ('times', 2),\n",
       " ('it', 4),\n",
       " ('was', 4),\n",
       " ('the', 4),\n",
       " ('age', 2),\n",
       " ('of', 4),\n",
       " ('wisdom', 1),\n",
       " ('it', 4),\n",
       " ('was', 4),\n",
       " ('the', 4),\n",
       " ('age', 2),\n",
       " ('of', 4),\n",
       " ('foolishness', 1)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(wordlist, wordfreq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a list of words, return a dictionary of\n",
    "# word-frequency pairs.\n",
    "\n",
    "def wordListToFreqDict(wordlist):\n",
    "    wordfreq = [wordlist.count(p) for p in wordlist]\n",
    "    return dict(zip(wordlist,wordfreq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort a dictionary of word-frequency pairs in\n",
    "# order of descending frequency.\n",
    "\n",
    "def sortFreqDict(freqdict):\n",
    "    aux = [(freqdict[key], key) for key in freqdict]\n",
    "    aux.sort()\n",
    "    aux.reverse()\n",
    "    return aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = ['a', 'about', 'above', 'across', 'after', 'afterwards']\n",
    "stopwords += ['again', 'against', 'all', 'almost', 'alone', 'along']\n",
    "stopwords += ['already', 'also', 'although', 'always', 'am', 'among']\n",
    "stopwords += ['amongst', 'amoungst', 'amount', 'an', 'and', 'another']\n",
    "stopwords += ['any', 'anyhow', 'anyone', 'anything', 'anyway', 'anywhere']\n",
    "stopwords += ['are', 'around', 'as', 'at', 'back', 'be', 'became']\n",
    "stopwords += ['because', 'become', 'becomes', 'becoming', 'been']\n",
    "stopwords += ['before', 'beforehand', 'behind', 'being', 'below']\n",
    "stopwords += ['beside', 'besides', 'between', 'beyond', 'bill', 'both']\n",
    "stopwords += ['bottom', 'but', 'by', 'call', 'can', 'cannot', 'cant']\n",
    "stopwords += ['co', 'computer', 'con', 'could', 'couldnt', 'cry', 'de']\n",
    "stopwords += ['describe', 'detail', 'did', 'do', 'done', 'down', 'due']\n",
    "stopwords += ['during', 'each', 'eg', 'eight', 'either', 'eleven', 'else']\n",
    "stopwords += ['elsewhere', 'empty', 'enough', 'etc', 'even', 'ever']\n",
    "stopwords += ['every', 'everyone', 'everything', 'everywhere', 'except']\n",
    "stopwords += ['few', 'fifteen', 'fifty', 'fill', 'find', 'fire', 'first']\n",
    "stopwords += ['five', 'for', 'former', 'formerly', 'forty', 'found']\n",
    "stopwords += ['four', 'from', 'front', 'full', 'further', 'get', 'give']\n",
    "stopwords += ['go', 'had', 'has', 'hasnt', 'have', 'he', 'hence', 'her']\n",
    "stopwords += ['here', 'hereafter', 'hereby', 'herein', 'hereupon', 'hers']\n",
    "stopwords += ['herself', 'him', 'himself', 'his', 'how', 'however']\n",
    "stopwords += ['hundred', 'i', 'ie', 'if', 'in', 'inc', 'indeed']\n",
    "stopwords += ['interest', 'into', 'is', 'it', 'its', 'itself', 'keep']\n",
    "stopwords += ['last', 'latter', 'latterly', 'least', 'less', 'ltd', 'made']\n",
    "stopwords += ['many', 'may', 'me', 'meanwhile', 'might', 'mill', 'mine']\n",
    "stopwords += ['more', 'moreover', 'most', 'mostly', 'move', 'much']\n",
    "stopwords += ['must', 'my', 'myself', 'name', 'namely', 'neither', 'never']\n",
    "stopwords += ['nevertheless', 'next', 'nine', 'no', 'nobody', 'none']\n",
    "stopwords += ['noone', 'nor', 'not', 'nothing', 'now', 'nowhere', 'of']\n",
    "stopwords += ['off', 'often', 'on','once', 'one', 'only', 'onto', 'or']\n",
    "stopwords += ['other', 'others', 'otherwise', 'our', 'ours', 'ourselves']\n",
    "stopwords += ['out', 'over', 'own', 'part', 'per', 'perhaps', 'please']\n",
    "stopwords += ['put', 'rather', 're', 's', 'same', 'see', 'seem', 'seemed']\n",
    "stopwords += ['seeming', 'seems', 'serious', 'several', 'she', 'should']\n",
    "stopwords += ['show', 'side', 'since', 'sincere', 'six', 'sixty', 'so']\n",
    "stopwords += ['some', 'somehow', 'someone', 'something', 'sometime']\n",
    "stopwords += ['sometimes', 'somewhere', 'still', 'such', 'system', 'take']\n",
    "stopwords += ['ten', 'than', 'that', 'the', 'their', 'them', 'themselves']\n",
    "stopwords += ['then', 'thence', 'there', 'thereafter', 'thereby']\n",
    "stopwords += ['therefore', 'therein', 'thereupon', 'these', 'they']\n",
    "stopwords += ['thick', 'thin', 'third', 'this', 'those', 'though', 'three']\n",
    "stopwords += ['three', 'through', 'throughout', 'thru', 'thus', 'to']\n",
    "stopwords += ['together', 'too', 'top', 'toward', 'towards', 'twelve']\n",
    "stopwords += ['twenty', 'two', 'un', 'under', 'until', 'up', 'upon']\n",
    "stopwords += ['us', 'very', 'via', 'was', 'we', 'well', 'were', 'what']\n",
    "stopwords += ['whatever', 'when', 'whence', 'whenever', 'where']\n",
    "stopwords += ['whereafter', 'whereas', 'whereby', 'wherein', 'whereupon']\n",
    "stopwords += ['wherever', 'whether', 'which', 'while', 'whither', 'who']\n",
    "stopwords += ['whoever', 'whole', 'whom', 'whose', 'why', 'will', 'with']\n",
    "stopwords += ['within', 'without', 'would', 'yet', 'you', 'your']\n",
    "stopwords += ['yours', 'yourself', 'yourselves']\n",
    "\n",
    "# Given a list of words, remove any that are\n",
    "# in a list of stop words.\n",
    "\n",
    "def removeStopwords(wordlist, stopwords):\n",
    "    return [w for w in wordlist if w not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N GRAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a list of n-grams identify the index of the keyword.\n",
    "\n",
    "def nGramsToKWICDict(ngrams):\n",
    "    keyindex = len(ngrams[0]) // 2\n",
    "\n",
    "    return keyindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'obo' has no attribute 'nGramsToKWICDict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-63c1aa063aa3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mngrams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetNGrams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnGramsToKWICDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngrams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'obo' has no attribute 'nGramsToKWICDict'"
     ]
    }
   ],
   "source": [
    "#get-keyword.py\n",
    "\n",
    "import obo\n",
    "\n",
    "test = 'this test sentence has eight words in it'\n",
    "ngrams = obo.getNGrams(test.split(), 5)\n",
    "\n",
    "print(obo.nGramsToKWICDict(ngrams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a list of n-grams, return a dictionary of KWICs,\n",
    "# indexed by keyword.\n",
    "\n",
    "def nGramsToKWICDict(ngrams):\n",
    "    keyindex = len(ngrams[0]) // 2\n",
    "\n",
    "    kwicdict = {}\n",
    "\n",
    "    for k in ngrams:\n",
    "        if k[keyindex] not in kwicdict:\n",
    "            kwicdict[k[keyindex]] = [k]\n",
    "        else:\n",
    "            kwicdict[k[keyindex]].append(k)\n",
    "    return kwicdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentence': [['this', 'test', 'sentence', 'has', 'eight']], 'words': [['has', 'eight', 'words', 'in', 'it']], 'has': [['test', 'sentence', 'has', 'eight', 'words']], 'eight': [['sentence', 'has', 'eight', 'words', 'in']]}\n"
     ]
    }
   ],
   "source": [
    "print(nGramsToKWICDict(ngrams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # html-to-pretty-print.py\n",
    "# import obo\n",
    "\n",
    "# # create dictionary of n-grams\n",
    "# n = 7\n",
    "# url = 'http://www.oldbaileyonline.org/browse.jsp?id=t17800628-33&div=t17800628-33'\n",
    "\n",
    "# text = obo.webPageToText(url)\n",
    "# fullwordlist = obo.stripNonAlphaNum(text)\n",
    "# ngrams = obo.getNGrams(fullwordlist, n)\n",
    "# worddict = obo.nGramsToKWICDict(ngrams)\n",
    "\n",
    "# print(worddict[\"black\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # calculate the length of the n-gram\n",
    "# kwic = 'amongst them a black there was one'.split()\n",
    "# n = len(kwic)\n",
    "# print(n)\n",
    "# -> 7\n",
    "\n",
    "# # calculate the index position of the keyword\n",
    "# keyindex = n // 2\n",
    "# print(keyindex)\n",
    "# -> 3\n",
    "\n",
    "# # display the items before the keyword\n",
    "# print(kwic[:keyindex])\n",
    "# -> ['amongst', 'them', 'a']\n",
    "\n",
    "# # display the keyword only\n",
    "# print(kwic[keyindex])\n",
    "# -> black\n",
    "\n",
    "# # display the items after the keyword\n",
    "# print(kwic[(keyindex+1):])\n",
    "# -> ['there', 'was', 'one']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['it', 'was', 'the', 'best']\n",
      "['it', 'was', 'the', 'best', 'of', 'times']\n",
      "['it', 'was', 'the', 'worst']\n",
      "['it', 'was', 'the', 'best', 'of', 'times', 'it', 'was', 'the', 'worst', 'of', 'times']\n",
      "['it', 'was', 'the', 'best', 'of', 'times', 'it', 'was', 'the', 'worst', 'of', 'times']\n",
      "['it', 'was', 'the', 'age', 'of', 'wisdom', 'it', 'was', 'the', 'age', 'of', 'foolishness']\n"
     ]
    }
   ],
   "source": [
    "wordstring = 'it was the best of times it was the worst of times '\n",
    "wordstring += 'it was the age of wisdom it was the age of foolishness'\n",
    "wordlist = wordstring.split()\n",
    "\n",
    "print(wordlist[0:4])\n",
    "# -> ['it', 'was', 'the', 'best']\n",
    "\n",
    "print(wordlist[0:6])\n",
    "# -> ['it', 'was', 'the', 'best', 'of', 'times']\n",
    "\n",
    "print(wordlist[6:10])\n",
    "# -> ['it', 'was', 'the', 'worst']\n",
    "\n",
    "print(wordlist[0:12])\n",
    "# -> ['it', 'was', 'the', 'best', 'of', 'times', 'it', 'was', 'the', 'worst', 'of', 'times']\n",
    "\n",
    "print(wordlist[:12])\n",
    "# -> ['it', 'was', 'the', 'best', 'of', 'times', 'it', 'was', 'the', 'worst', 'of', 'times']\n",
    "\n",
    "print(wordlist[12:])\n",
    "# -> ['it', 'was', 'the', 'age', 'of', 'wisdom', 'it', 'was', 'the', 'age', 'of', 'foolishness']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['it', 'was', 'the', 'best', 'of']\n",
      "['was', 'the', 'best', 'of', 'times']\n",
      "['the', 'best', 'of', 'times', 'it']\n",
      "['best', 'of', 'times', 'it', 'was']\n",
      "['of', 'times', 'it', 'was', 'the']\n",
      "['times', 'it', 'was', 'the', 'worst']\n",
      "['it', 'was', 'the', 'worst', 'of']\n",
      "['was', 'the', 'worst', 'of', 'times']\n",
      "['the', 'worst', 'of', 'times', 'it']\n",
      "['worst', 'of', 'times', 'it', 'was']\n",
      "['of', 'times', 'it', 'was', 'the']\n",
      "['times', 'it', 'was', 'the', 'age']\n",
      "['it', 'was', 'the', 'age', 'of']\n",
      "['was', 'the', 'age', 'of', 'wisdom']\n",
      "['the', 'age', 'of', 'wisdom', 'it']\n",
      "['age', 'of', 'wisdom', 'it', 'was']\n",
      "['of', 'wisdom', 'it', 'was', 'the']\n",
      "['wisdom', 'it', 'was', 'the', 'age']\n",
      "['it', 'was', 'the', 'age', 'of']\n",
      "['was', 'the', 'age', 'of', 'foolishness']\n",
      "['the', 'age', 'of', 'foolishness']\n",
      "['age', 'of', 'foolishness']\n",
      "['of', 'foolishness']\n",
      "['foolishness']\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for items in wordlist:\n",
    "    print(wordlist[i: i+5])\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a list of words and a number n, return a list\n",
    "# of n-grams.\n",
    "\n",
    "def getNGrams(wordlist, n):\n",
    "    return [wordlist[i:i+n] for i in range(len(wordlist)-(n-1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNGrams(wordlist, n):\n",
    "    ngrams = []\n",
    "    for i in range(len(wordlist)-(n-1)):\n",
    "        ngrams.append(wordlist[i:i+n])\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['it', 'was', 'the', 'best', 'of'], ['was', 'the', 'best', 'of', 'times'], ['the', 'best', 'of', 'times', 'it'], ['best', 'of', 'times', 'it', 'was'], ['of', 'times', 'it', 'was', 'the'], ['times', 'it', 'was', 'the', 'worst'], ['it', 'was', 'the', 'worst', 'of'], ['was', 'the', 'worst', 'of', 'times'], ['the', 'worst', 'of', 'times', 'it'], ['worst', 'of', 'times', 'it', 'was'], ['of', 'times', 'it', 'was', 'the'], ['times', 'it', 'was', 'the', 'age'], ['it', 'was', 'the', 'age', 'of'], ['was', 'the', 'age', 'of', 'wisdom'], ['the', 'age', 'of', 'wisdom', 'it'], ['age', 'of', 'wisdom', 'it', 'was'], ['of', 'wisdom', 'it', 'was', 'the'], ['wisdom', 'it', 'was', 'the', 'age'], ['it', 'was', 'the', 'age', 'of'], ['was', 'the', 'age', 'of', 'foolishness']]\n"
     ]
    }
   ],
   "source": [
    "#useGetNGrams.py\n",
    "\n",
    "import obo\n",
    "\n",
    "wordstring = 'it was the best of times it was the worst of times '\n",
    "wordstring += 'it was the age of wisdom it was the age of foolishness'\n",
    "allMyWords = wordstring.split()\n",
    "\n",
    "print(obo.getNGrams(allMyWords, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test1 = 'here are four words'\n",
    "test2 = 'this test sentence has eight words in it'\n",
    "\n",
    "getNGrams(test1.split(), 5)\n",
    "# -> []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['this', 'test', 'sentence', 'has', 'eight'],\n",
       " ['test', 'sentence', 'has', 'eight', 'words'],\n",
       " ['sentence', 'has', 'eight', 'words', 'in'],\n",
       " ['has', 'eight', 'words', 'in', 'it']]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getNGrams(test2.split(), 5)\n",
    "# -> [['this', 'test', 'sentence', 'has', 'eight'],\n",
    "# ['test', 'sentence', 'has', 'eight', 'words'],\n",
    "# ['sentence', 'has', 'eight', 'words', 'in'],\n",
    "# ['has', 'eight', 'words', 'in', 'it']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    " tweet='We have some delightful new food in the cafeteria. Awesome!!!'\n",
    " positive_words=['awesome','good', 'nice', 'super', 'fun', 'delightful']\n",
    " negative_words=['awful','lame','horrible','bad']\n",
    " words= tweet.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delightful  is a positive word\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "     if word in positive_words:\n",
    "         print (word+'  is a positive word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'we have some delightful new food in the cafeteria. awesome!!!'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we\n",
      "have\n",
      "some\n",
      "delightful\n",
      "new\n",
      "food\n",
      "in\n",
      "the\n",
      "cafeteria.\n",
      "awesome!!!\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "     print (word.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delightful is a positive word\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "     if word.lower() in positive_words:\n",
    "         print (word.lower()+' is a positive word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have some delightful new food in the cafeteria. Awesome\n"
     ]
    }
   ],
   "source": [
    "print( tweet.replace('!',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have some delightful new food in the cafeteria. Awesome\n"
     ]
    }
   ],
   "source": [
    "tweet_noex=tweet.replace('!','')\n",
    "print (tweet_noex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have some delightful new food in the cafeteria. Awesome\n"
     ]
    }
   ],
   "source": [
    "tweet=tweet.replace('!','')\n",
    "print (tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have some delightful new food in the cafeteria Awesome\n"
     ]
    }
   ],
   "source": [
    "tweet_processed=tweet.replace('!','').replace('.','')\n",
    "print( tweet_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['we', 'have', 'some', 'delightful', 'new', 'food', 'in', 'the', 'cafeteria', 'awesome']\n"
     ]
    }
   ],
   "source": [
    " tweet_processed=tweet.replace('!','').replace('.','')\n",
    " tweet_processed=tweet_processed.lower()\n",
    " words=tweet_processed.split(' ')\n",
    " print (words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    " from string import punctuation\n",
    " print (punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have some delightful new food in the cafeteria awesome\n"
     ]
    }
   ],
   "source": [
    " tweet_processed=tweet.lower()\n",
    " for p in list(punctuation):\n",
    "     tweet_processed=tweet_processed.replace(p,'')\n",
    " \n",
    " print (tweet_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delightful is a positive word\n",
      "awesome is a positive word\n"
     ]
    }
   ],
   "source": [
    " for word in words:\n",
    "     if word in positive_words:\n",
    "         print (word + ' is a positive word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have some delightful new food in the cafeteria awesome\n"
     ]
    }
   ],
   "source": [
    " positive_counter=0\n",
    " tweet_processed=tweet.lower()\n",
    " for p in list(punctuation):\n",
    "     tweet_processed=tweet_processed.replace(p,'')\n",
    " \n",
    " print (tweet_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delightful is a positive word\n",
      "awesome is a positive word\n"
     ]
    }
   ],
   "source": [
    " for word in words:\n",
    "     if word in positive_words:\n",
    "         print (word+ ' is a positive word')\n",
    "         positive_counter=positive_counter+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    " print (positive_counter) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_counter/len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
