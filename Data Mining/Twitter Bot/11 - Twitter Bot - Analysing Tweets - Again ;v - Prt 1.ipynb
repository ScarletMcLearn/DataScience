{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mylist = [1,2,3]\n",
    "\n",
    "with open('Dataset/parrot.pkl', 'wb') as f:\n",
    "   pickle.dump(mylist, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('depression_1.pkle', 'rb') as f:\n",
    "    mynewlist = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "199219"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mynewlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/scarlet/.local/share/virtualenvs/installers-OifosnB-/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import hickle as hkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "while(i < 2001):\n",
    "    if (i % 1000 == 0):\n",
    "        print(i)\n",
    "    i = i + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "from contractions import CONTRACTION_MAP\n",
    "\n",
    "import nltk\n",
    "\n",
    "from textstat.textstat import textstat\n",
    "\n",
    "import obo\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "import preprocessor as p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'preprocessor' from '/home/scarlet/.local/share/virtualenvs/installers-OifosnB-/lib/python3.5/site-packages/preprocessor/__init__.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCleaner:\n",
    "    '''Cleans text for Analysis.'''\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    \n",
    "    def cln_twt(twt):\n",
    "        '''Get text from tweet'''\n",
    "        return p.clean(twt)\n",
    "    \n",
    "    def tkn_twt(twt):\n",
    "        '''Tokenize tweet'''\n",
    "        return p.tokenize(twt)\n",
    "    \n",
    "    def prs_twt(twt):\n",
    "        '''Get text from tweet'''\n",
    "        return p.parse(twt)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def clean_tweet(tweet):\n",
    "        '''\n",
    "        Utility function to clean the text in a tweet by removing \n",
    "        links and special characters using regex.\n",
    "        '''\n",
    "        return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", tweet).split())\n",
    "    \n",
    "    \n",
    "    \n",
    "    def removeStopwords(wordlist):\n",
    "        '''\n",
    "        removeStopwords(\"this is a test sentence.\".split(\" \"))\n",
    "        >>> ['test', 'sentence.']\n",
    "        '''\n",
    "        return [w for w in wordlist if w not in nltk.corpus.stopwords.words('english')]\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    def remove_characters_before_tokenization(self, sentence,\n",
    "         keep_apostrophes=False):\n",
    "        \n",
    "         '''\n",
    "         remove_characters_before_tokenization('Test sentence 1 2 3 4 @')\n",
    "         >>> Test sentence 1 2 3 4\n",
    "         '''\n",
    "        \n",
    "         sentence = sentence.strip()\n",
    "         if keep_apostrophes:\n",
    "             PATTERN = r'[?|$|&|*|%|@|(|)|~]' # add other characters here to remove them\n",
    "             filtered_sentence = re.sub(PATTERN, r'', sentence)\n",
    "         else:\n",
    "             PATTERN = r'[^a-zA-Z0-9 ]' # only extract alpha-numeric characters\n",
    "             filtered_sentence = re.sub(PATTERN, r'', sentence)\n",
    "         return filtered_sentence \n",
    "    \n",
    "    \n",
    "    \n",
    "    def expand_contractions(self, sentence, contraction_mapping):\n",
    "         '''\n",
    "          expand_contractions(\"This is a test sentence. But this isn't one. hasn't she hearten?\", CONTRACTION_MAP)\n",
    "          >>> This is a test sentence. But this is not one. has not she hearten?\n",
    "         '''\n",
    "         contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())),\n",
    "         flags=re.IGNORECASE|re.DOTALL)\n",
    "            \n",
    "         def expand_match(contraction):\n",
    "             match = contraction.group(0)\n",
    "             first_char = match[0]\n",
    "             expanded_contraction = contraction_mapping.get(match)\\\n",
    "                 if contraction_mapping.get(match)\\\n",
    "                 else contraction_mapping.get(match.lower())\n",
    "             expanded_contraction = first_char+expanded_contraction[1:]\n",
    "             return expanded_contraction\n",
    "         expanded_sentence = contractions_pattern.sub(expand_match, sentence)\n",
    "         return expanded_sentence \n",
    "        \n",
    "        \n",
    "        \n",
    "    def clean_tweet(tweet):\n",
    "        '''\n",
    "        Utility function to clean the text in a tweet by removing \n",
    "        links and special characters using regex.\n",
    "        '''\n",
    "        return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", tweet).split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextAnalyser:\n",
    "    '''Analyses text.'''\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def stat_text_analysis(self, test_data):\n",
    "        '''\n",
    "        Returns list of statistical text test results.\n",
    "        '''\n",
    "        fre = textstat.flesch_reading_ease(test_data)\n",
    "        si = textstat.smog_index(test_data)\n",
    "        fkg = textstat.flesch_kincaid_grade(test_data)\n",
    "        cli = textstat.coleman_liau_index(test_data)\n",
    "        ari = textstat.automated_readability_index(test_data)\n",
    "        dcri = textstat.dale_chall_readability_score(test_data)\n",
    "        dw = textstat.difficult_words(test_data)\n",
    "        lwf = textstat.linsear_write_formula(test_data)\n",
    "        gf = textstat.gunning_fog(test_data)\n",
    "        ts = textstat.text_standard(test_data)\n",
    "        \n",
    "        return ([\n",
    "            ['fre', fre],\n",
    "            ['si', si],\n",
    "            ['fkg', fkg],\n",
    "            ['cli', cli],\n",
    "            ['ari', ari],\n",
    "            ['dcri', dcri],\n",
    "            ['dw', dw],\n",
    "            ['lwf', lwf],\n",
    "            ['gf', gf],\n",
    "            ['ts', ts],            \n",
    "                ])\n",
    "    \n",
    "    \n",
    "    \n",
    "    def word_freq_pair(wordstring):\n",
    "        '''\n",
    "        print((word_freq_pair('this is a test sentence is is is.')))\n",
    "        [('this', 1), ('is', 3), ('a', 1), ('test', 1), ('sentence', 1), ('is', 3), ('is', 3), ('is.', 1)]\n",
    "        '''\n",
    "        wordlist = wordstring.split()\n",
    "\n",
    "        wordfreq = []\n",
    "\n",
    "        for w in wordlist:\n",
    "            wordfreq.append(wordlist.count(w))\n",
    "\n",
    "        return list(zip(wordlist, wordfreq))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    " # WILL NEED TO TEST   \n",
    "\n",
    "    def nGramsToKWICDict(ngrams):\n",
    "        '''\n",
    "        ngrams = obo.getNGrams('this test sentence has eight words in it'.split(), 5)\n",
    "        print(ngrams)\n",
    "        >>> [['this', 'test', 'sentence', 'has', 'eight'],\n",
    "             ['test', 'sentence', 'has', 'eight', 'words'],\n",
    "             ['sentence', 'has', 'eight', 'words', 'in'],\n",
    "             ['has', 'eight', 'words', 'in', 'it']]\n",
    "        '''\n",
    "        keyindex = len(ngrams[0]) // 2\n",
    "\n",
    "        return keyindex\n",
    "    \n",
    "    \n",
    "    def nGramsToKWICDict(ngrams):\n",
    "        '''\n",
    "        print(nGramsToKWICDict(\n",
    "                             [['this', 'test', 'sentence', 'has', 'eight'],\n",
    "                             ['test', 'sentence', 'has', 'eight', 'words'],\n",
    "                             ['sentence', 'has', 'eight', 'words', 'in'],\n",
    "                             ['has', 'eight', 'words', 'in', 'it']]\n",
    "                             ))\n",
    "                             \n",
    "        >>> {'words': [['has', 'eight', 'words', 'in', 'it']], \n",
    "            'sentence': [['this', 'test', 'sentence', 'has', 'eight']], \n",
    "            'has': [['test', 'sentence', 'has', 'eight', 'words']], \n",
    "            'eight': [['sentence', 'has', 'eight', 'words', 'in']]}\n",
    "        \n",
    "        '''\n",
    "        keyindex = len(ngrams[0]) // 2\n",
    "\n",
    "        kwicdict = {}\n",
    "\n",
    "        for k in ngrams:\n",
    "            if k[keyindex] not in kwicdict:\n",
    "                kwicdict[k[keyindex]] = [k]\n",
    "            else:\n",
    "                kwicdict[k[keyindex]].append(k)\n",
    "        return kwicdict\n",
    "    \n",
    "    def get_n_grams(allMyWords, n):\n",
    "        return obo.getNGrams(allMyWords, n)\n",
    "    \n",
    "    def getNGrams(wordlist, n):\n",
    "        '''\n",
    "        tNGrams(test2.split(), 5)\n",
    "        # -> [['this', 'test', 'sentence', 'has', 'eight'],\n",
    "        # ['test', 'sentence', 'has', 'eight', 'words'],\n",
    "        # ['sentence', 'has', 'eight', 'words', 'in'],\n",
    "        # ['has', 'eight', 'words', 'in', 'it']]\n",
    "        '''\n",
    "        ngrams = []\n",
    "        for i in range(len(wordlist)-(n-1)):\n",
    "            ngrams.append(wordlist[i:i+n])\n",
    "        return ngrams\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def analize_sentiment(tweet):\n",
    "        '''\n",
    "        Utility function to classify the polarity of a tweet\n",
    "        using textblob.\n",
    "        '''\n",
    "        analysis = TextBlob(clean_tweet(tweet))\n",
    "        if analysis.sentiment.polarity > 0:\n",
    "            return 1\n",
    "        elif analysis.sentiment.polarity == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return -1\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
