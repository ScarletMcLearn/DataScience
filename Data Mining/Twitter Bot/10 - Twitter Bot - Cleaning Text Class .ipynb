{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "from contractions import CONTRACTION_MAP\n",
    "\n",
    "import nltk\n",
    "\n",
    "from textstat.textstat import textstat\n",
    "\n",
    "import obo\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "import preprocessor as p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCleaner:\n",
    "    '''Cleans text for Analysis.'''\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def remove_characters_before_tokenization(self, sentence,\n",
    "         keep_apostrophes=False):\n",
    "        \n",
    "         '''\n",
    "         remove_characters_before_tokenization('Test sentence 1 2 3 4 @')\n",
    "         >>> Test sentence 1 2 3 4\n",
    "         '''\n",
    "        \n",
    "         sentence = sentence.strip()\n",
    "         if keep_apostrophes:\n",
    "             PATTERN = r'[?|$|&|*|%|@|(|)|~]' # add other characters here to remove them\n",
    "             filtered_sentence = re.sub(PATTERN, r'', sentence)\n",
    "         else:\n",
    "             PATTERN = r'[^a-zA-Z0-9 ]' # only extract alpha-numeric characters\n",
    "             filtered_sentence = re.sub(PATTERN, r'', sentence)\n",
    "         return filtered_sentence \n",
    "    \n",
    "    def expand_contractions(self, sentence, contraction_mapping):\n",
    "         '''\n",
    "          expand_contractions(\"This is a test sentence. But this isn't one. hasn't she hearten?\", CONTRACTION_MAP)\n",
    "          >>> This is a test sentence. But this is not one. has not she hearten?\n",
    "         '''\n",
    "         contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())),\n",
    "         flags=re.IGNORECASE|re.DOTALL)\n",
    "            \n",
    "         def expand_match(contraction):\n",
    "             match = contraction.group(0)\n",
    "             first_char = match[0]\n",
    "             expanded_contraction = contraction_mapping.get(match)\\\n",
    "                 if contraction_mapping.get(match)\\\n",
    "                 else contraction_mapping.get(match.lower())\n",
    "             expanded_contraction = first_char+expanded_contraction[1:]\n",
    "             return expanded_contraction\n",
    "         expanded_sentence = contractions_pattern.sub(expand_match, sentence)\n",
    "         return expanded_sentence \n",
    "        \n",
    "    def remove_stopwords(tokens):\n",
    "         '''\n",
    "        remove_stopwords(nltk.word_tokenize(\"This is a test sentence. But this isn't one. hasn't she hearten?\".lower()))\n",
    "        >>> ['test', 'sentence', '.', \"n't\", 'one', '.', \"n't\", 'hearten', '?']\n",
    "         '''\n",
    "         stopword_list = nltk.corpus.stopwords.words('english')\n",
    "         filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "         return filtered_tokens \n",
    "        \n",
    "    def stat_text_analysis(self, test_data):\n",
    "        '''\n",
    "        Returns list of statistical text test results.\n",
    "        '''\n",
    "        fre = textstat.flesch_reading_ease(test_data)\n",
    "        si = textstat.smog_index(test_data)\n",
    "        fkg = textstat.flesch_kincaid_grade(test_data)\n",
    "        cli = textstat.coleman_liau_index(test_data)\n",
    "        ari = textstat.automated_readability_index(test_data)\n",
    "        dcri = textstat.dale_chall_readability_score(test_data)\n",
    "        dw = textstat.difficult_words(test_data)\n",
    "        lwf = textstat.linsear_write_formula(test_data)\n",
    "        gf = textstat.gunning_fog(test_data)\n",
    "        ts = textstat.text_standard(test_data)\n",
    "        \n",
    "        return ([\n",
    "            ['fre', fre],\n",
    "            ['si', si],\n",
    "            ['fkg', fkg],\n",
    "            ['cli', cli],\n",
    "            ['ari', ari],\n",
    "            ['dcri', dcri],\n",
    "            ['dw', dw],\n",
    "            ['lwf', lwf],\n",
    "            ['gf', gf],\n",
    "            ['ts', ts],            \n",
    "                ])\n",
    "    \n",
    "    def word_freq_pair(wordstring):\n",
    "        '''\n",
    "        print((word_freq_pair('this is a test sentence is is is.')))\n",
    "        [('this', 1), ('is', 3), ('a', 1), ('test', 1), ('sentence', 1), ('is', 3), ('is', 3), ('is.', 1)]\n",
    "        '''\n",
    "        wordlist = wordstring.split()\n",
    "\n",
    "        wordfreq = []\n",
    "\n",
    "        for w in wordlist:\n",
    "            wordfreq.append(wordlist.count(w))\n",
    "\n",
    "        return list(zip(wordlist, wordfreq))\n",
    "    \n",
    "    \n",
    "    def removeStopwords(wordlist):\n",
    "        '''\n",
    "        removeStopwords(\"this is a test sentence.\".split(\" \"))\n",
    "        >>> ['test', 'sentence.']\n",
    "        '''\n",
    "        return [w for w in wordlist if w not in nltk.corpus.stopwords.words('english')]\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    def nGramsToKWICDict(ngrams):\n",
    "        '''\n",
    "        ngrams = obo.getNGrams('this test sentence has eight words in it'.split(), 5)\n",
    "        print(ngrams)\n",
    "        >>> [['this', 'test', 'sentence', 'has', 'eight'],\n",
    "             ['test', 'sentence', 'has', 'eight', 'words'],\n",
    "             ['sentence', 'has', 'eight', 'words', 'in'],\n",
    "             ['has', 'eight', 'words', 'in', 'it']]\n",
    "        '''\n",
    "        keyindex = len(ngrams[0]) // 2\n",
    "\n",
    "        return keyindex\n",
    "    \n",
    "    \n",
    "    def nGramsToKWICDict(ngrams):\n",
    "        '''\n",
    "        print(nGramsToKWICDict(\n",
    "                             [['this', 'test', 'sentence', 'has', 'eight'],\n",
    "                             ['test', 'sentence', 'has', 'eight', 'words'],\n",
    "                             ['sentence', 'has', 'eight', 'words', 'in'],\n",
    "                             ['has', 'eight', 'words', 'in', 'it']]\n",
    "                             ))\n",
    "                             \n",
    "        >>> {'words': [['has', 'eight', 'words', 'in', 'it']], \n",
    "            'sentence': [['this', 'test', 'sentence', 'has', 'eight']], \n",
    "            'has': [['test', 'sentence', 'has', 'eight', 'words']], \n",
    "            'eight': [['sentence', 'has', 'eight', 'words', 'in']]}\n",
    "        \n",
    "        '''\n",
    "        keyindex = len(ngrams[0]) // 2\n",
    "\n",
    "        kwicdict = {}\n",
    "\n",
    "        for k in ngrams:\n",
    "            if k[keyindex] not in kwicdict:\n",
    "                kwicdict[k[keyindex]] = [k]\n",
    "            else:\n",
    "                kwicdict[k[keyindex]].append(k)\n",
    "        return kwicdict\n",
    "    \n",
    "    def get_n_grams(allMyWords, n):\n",
    "        return obo.getNGrams(allMyWords, n)\n",
    "    \n",
    "    def getNGrams(wordlist, n):\n",
    "        '''\n",
    "        tNGrams(test2.split(), 5)\n",
    "        # -> [['this', 'test', 'sentence', 'has', 'eight'],\n",
    "        # ['test', 'sentence', 'has', 'eight', 'words'],\n",
    "        # ['sentence', 'has', 'eight', 'words', 'in'],\n",
    "        # ['has', 'eight', 'words', 'in', 'it']]\n",
    "        '''\n",
    "        ngrams = []\n",
    "        for i in range(len(wordlist)-(n-1)):\n",
    "            ngrams.append(wordlist[i:i+n])\n",
    "        return ngrams\n",
    "    \n",
    "    \n",
    "    \n",
    "    def clean_tweet(tweet):\n",
    "        '''\n",
    "        Utility function to clean the text in a tweet by removing \n",
    "        links and special characters using regex.\n",
    "        '''\n",
    "        return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", tweet).split())\n",
    "\n",
    "    def analize_sentiment(tweet):\n",
    "        '''\n",
    "        Utility function to classify the polarity of a tweet\n",
    "        using textblob.\n",
    "        '''\n",
    "        analysis = TextBlob(clean_tweet(tweet))\n",
    "        if analysis.sentiment.polarity > 0:\n",
    "            return 1\n",
    "        elif analysis.sentiment.polarity == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return -1\n",
    "        \n",
    "    def cln_twt(twt):\n",
    "        return p.clean(twt)\n",
    "    \n",
    "    def tkn_twt(twt):\n",
    "        return p.tokenize(twt)\n",
    "    \n",
    "    def prs_twt(twt):\n",
    "        return p.parse(twt)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on textstatistics in module textstat.textstat object:\n",
      "\n",
      "class textstatistics(builtins.object)\n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  automated_readability_index(*args, **kwargs)\n",
      " |  \n",
      " |  avg_letter_per_word(*args, **kwargs)\n",
      " |  \n",
      " |  avg_sentence_length(*args, **kwargs)\n",
      " |  \n",
      " |  avg_sentence_per_word(*args, **kwargs)\n",
      " |  \n",
      " |  avg_syllables_per_word(*args, **kwargs)\n",
      " |  \n",
      " |  char_count(*args, **kwargs)\n",
      " |      Function to return total character counts in a text, pass the following parameter\n",
      " |      ignore_spaces = False\n",
      " |      to ignore whitespaces\n",
      " |  \n",
      " |  coleman_liau_index(*args, **kwargs)\n",
      " |  \n",
      " |  dale_chall_readability_score(*args, **kwargs)\n",
      " |  \n",
      " |  difficult_words(*args, **kwargs)\n",
      " |  \n",
      " |  flesch_kincaid_grade(*args, **kwargs)\n",
      " |  \n",
      " |  flesch_reading_ease(*args, **kwargs)\n",
      " |  \n",
      " |  gunning_fog(*args, **kwargs)\n",
      " |  \n",
      " |  lexicon_count(*args, **kwargs)\n",
      " |      Function to return total lexicon (words in lay terms) counts in a text\n",
      " |  \n",
      " |  linsear_write_formula(*args, **kwargs)\n",
      " |  \n",
      " |  lix(*args, **kwargs)\n",
      " |  \n",
      " |  polysyllabcount(*args, **kwargs)\n",
      " |  \n",
      " |  sentence_count(*args, **kwargs)\n",
      " |      Sentence count of a text\n",
      " |  \n",
      " |  smog_index(*args, **kwargs)\n",
      " |  \n",
      " |  syllable_count(*args, **kwargs)\n",
      " |      Function to calculate syllable words in a text.\n",
      " |      I/P - a text\n",
      " |      O/P - number of syllable words\n",
      " |  \n",
      " |  text_standard(*args, **kwargs)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(textstat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from string import punctuation\n",
    "\n",
    "# postivie_words = ['delightful', 'awesome']\n",
    "\n",
    "# def positivity(positive_words, text):\n",
    "#     tweet_processed = text\n",
    "#     positive_counter=0\n",
    "    \n",
    "#     tweet_processed=tweet.lower()\n",
    "    \n",
    "#     for p in list(punctuation):\n",
    "#          tweet_processed=tweet_processed.replace(p,'')\n",
    "    \n",
    "#     for word in tweet_processed.split(' '):\n",
    "#          if word in positive_words:\n",
    "# #              print (word+ ' is a positive word')\n",
    "#              positive_counter=positive_counter+1\n",
    "#     print(tweet_processed)\n",
    "\n",
    "#     return positive_counter/len(tweet_processed) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
